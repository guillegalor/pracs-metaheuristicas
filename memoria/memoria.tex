\documentclass[size=a4, parskip=half, titlepage=false, toc=flat, toc=bib, 12pt]{scrartcl}

\setuptoc{toc}{leveldown}

% Ajuste de las líneas y párrafos
\linespread{1.2}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% Español
\usepackage[spanish, es-tabla]{babel}

% Matemáticas
\usepackage{amsmath}
\usepackage{amsthm}

% Fuentes
\usepackage{newpxtext,newpxmath}
\usepackage[scale=.9]{FiraMono}
\usepackage{FiraSans}
\usepackage[T1]{fontenc}

\defaultfontfeatures{Ligatures=TeX,Numbers=Lining}
\usepackage[activate={true,nocompatibility},final,tracking=true,factor=1100,stretch=10,shrink=10]{microtype}
\SetTracking{encoding={*}, shape=sc}{0}

\usepackage{graphicx}
\usepackage{float}

% Mejores tablas
\usepackage{booktabs}

\usepackage{adjustbox}
% COLORES

\usepackage{xcolor}

\definecolor{verde}{HTML}{007D51}
\definecolor{esmeralda}{HTML}{045D56}
\definecolor{salmon}{HTML}{FF6859}
\definecolor{amarillo}{HTML}{FFAC12}
\definecolor{morado}{HTML}{A932FF}
\definecolor{azul}{HTML}{0082FB}
\definecolor{error}{HTML}{b00020}

% ENTORNOS
\usepackage[skins, listings, theorems]{tcolorbox}

\newtcolorbox{recuerda}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=black!10,
	lefttitle=0pt,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=black,
  title=\raisebox{-0.6ex}{\small RECUERDA}
}

\newtcolorbox{nota}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=black!10,
	lefttitle=0pt,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=black,
  title=\raisebox{-0.6ex}{\small NOTA}
}

\newtcolorbox{error}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=error!10,
	lefttitle=0pt,
  coltitle=error,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=error,
  title=\raisebox{-0.6ex}{\small ERROR}
}

\newtcblisting{shell}{
  enhanced,
  colback=black!10,
  colupper=black,
  frame hidden,
  opacityback=0,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  %titlerule=0.8mm,
  %titlerule style=black,
  %title=Consola,
  listing only,
  listing options={
    style=tcblatex,
    language=sh,
    breaklines=true,
    postbreak=\mbox{\textcolor{black}{$\hookrightarrow$}\space},
    emph={guille@guille-pc},
    emphstyle={\bfseries},
  },
}

\newtcbtheorem[number within=section]{teor}{\small TEOREMA}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{teor}

\newtcbtheorem[number within=section]{prop}{\small PROPOSICIÓN}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{prop}

\newtcbtheorem[number within=section]{cor}{\small COROLARIO}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{cor}

\newtcbtheorem[number within=section]{defi}{\small DEFINICIÓN}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{defi}

\newtcbtheorem{ejer}{\small EJERCICIO}{
  enhanced,
  sharp corners,
  frame hidden,
  left=0mm,
  right=0mm,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape,
  nameref/.style={},
}{ejer}

% CÓDIGO
\usepackage{listings}

% CABECERAS
\pagestyle{headings}
\setkomafont{pageheadfoot}{\normalfont\normalcolor\sffamily\small}
\setkomafont{pagenumber}{\normalfont\sffamily}

% ALGORITMOS
\usepackage[vlined,linesnumbered]{algorithm2e}

% Formato de los pies de figura
\setkomafont{captionlabel}{\scshape}
\SetAlCapFnt{\normalfont\scshape}
\SetAlgorithmName{Algoritmo}{Algoritmo}{Lista de algoritmos}

% BIBLIOGRAFÍA
%\usepackage[sorting=none]{biblatex}
%\addbibresource{bibliografia.bib}

\begin{document}

\renewcommand{\proofname}{\normalfont\sffamily\bfseries\small DEMOSTRACIÓN}

\title{Problema del Aprendizaje de Pesos}
\subject{Metaheurísticas}
\author{Guillermo Galindo Ortuño\\
    4 del Doble Grado en Informática y Matemáticas\\
    Guillegalor@gmail.com\\
    25619527b\\
    Problema 1.b APC}
\publishers{\vspace{2cm}\includegraphics[height=2.5cm]{UGR}\vspace{1cm}}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Descripción del Problema}

El objetivo de esta práctica es, a partir de un conjunto de muestras de elementos ya clasificados, construir un algoritmo que utilizando la información de la que dispone sea capaz de clasificar nuevos elementos. \\

Formalmente, dado un $n \in \mathbb{N}$, y un conjunto de clases $C$, un clasificador es una funció $f$ definida de  $\mathbb{R}^n$ en $C$ que asigna a cada vector $x \in \mathbb{R}^n$ una clase $f(x)$.\\

Nuestro problema consiste entonces en dada un conjunto finito de elementos ya clasificados $A = \{(x,c(x) \in \mathbb{R}^n \times C)$, encontrar un clasificador que permita clasificar otros elementos utilizando la información disponible en $A$. \\

En nuestro caso, partimos del clasificador 1-NN, que consiste en elegir la clase del elemento más cercano del conjunto ya conocido, utilizando usualmente la distancia euclídea. En particular, nosotros utilizaremos una distancia euclídea ponderada, que consiste en, dados dos vectores $u, v \in \mathbb{R}^n$ y un vector de pesos $w \in [0,1]^n$:
\[
d_w(u,v) = \sqrt{\sum_{i = 1}^{n}w*(u_i - v_i)^2}
.\]

Dicho esto, nuestro trabajo será estudiar diversos algoritmos que generen un vector de pesos y analizar como se comportan los respectivos clasificadores \newpage

\section{Consideraciones previas}
\subsection{Esquemas de representación}
\subsubsection{Práctica 1}
En primer lugar, para representar cada tipo de dato distinto utilizamos una interfaz llamada DataElem, de manera que cualquier estructura de datos que implemente DataElem deberá tener definidos los siguientes métodos:
\begin{itemize}
    \item \texttt{\textbf{fn} new()}. Función de clase que devuelve un elemento por defecto
    \item \texttt{\textbf{fn} get\_num\_attributes().} Función de clase que devuelve el número de atributos que tiene dicho tipo de dato.
    \item \texttt{\textbf{fn} get\_id()}  Función que devuelve un identificador de un elemento.
    \item \texttt{\textbf{fn} get\_class()} Función que devuelve la clase de un elemento.
    \item \texttt{\textbf{fn} get\_attr(i)} Función que devuelve el atributo i-ésimo de un elemento.
    \item \texttt{\textbf{fn} set\_id(i)} Función que asigna al identificador de un elemento el valor indicado en el parámetro \texttt{i}.
    \item \texttt{\textbf{fn} set\_class(c)} Función que asigna a la clase de un elemento el valor indicado en el parámetro \texttt{c}.
    \item \texttt{\textbf{fn} set\_attribute(i, v)} Función que asigna al atributo i-ésimo de un elemento el valor indicado en el parámetro \texttt{a}.
\end{itemize}

Dicho esto, hemos utilizado tres estructuras de datos que implementan esta interfaz:
\begin{itemize}
    \item \texttt{TextureRecord}, la cuál contiene un identificador, un array con 40 valores reales para almacenar los atributos, y un entero que representa la clase de dicho elemento.
    \item \texttt{ColposcopyRecord}, la cuál contiene un identificador, un array con 62 valores realespara almacenar los atributos, y un entero que representa la clase de dicho elemento.
    \item \texttt{TextureRecord}, la cuál contiene un identificador, un array con 34 valores reales para almacenar los atributos, y un entero que representa la clase de dicho elemento.
\end{itemize}
\subsubsection{Práctica 2}
En esta práctica hemos definido el tipo \texttt{Chromosome} que es un vector de pesos, y además hemos definido una estructura \texttt{ChromosomeAndResult} que contiene un \texttt{Chromosome} y una variable \texttt{result}, que es un flotante que almacena la evaluación de dicho vector de pesos. Sobre esta estructura definimos un orden que consiste en comparar los resultados de ambos elementos.
\subsection{Operadores comunes}
\subsubsection{Práctica 1}
Dados dos elementos de un tipo de dato que implemente dicha interfaz, hemos definido la función que calcula la \textbf{distancia euclídea} ponderada por un vector de pesos de la siguiente forma:

\begin{verbatim}

euclidean_distance_with_weigths(elem_1, elem_2, weights[]){
    distance = 0;

    for ind in 0 to num_attributtes {
        distance += (elem_1.attributes(ind) - elem_2.attributes(ind))
            * (elem_1.attributes(ind) - elem_2.attributes(ind))
            * weights[ind];
    }

    distance = square_root(distance);

    return distance;
}

\end{verbatim}
Por comodidad, cuando nos refiramos a la distancia euclídea usual omitiremos que llamamos a esta función con un vector de pesos con todos los valores iguales a 1 y únicamente escribiremos \texttt{euclidean\_distance} \\

Utilizando dicha distancia, definimos la función que actua como el \textbf{clasificador 1nn} así:
\begin{verbatim}
classifier_1nn_with_weights(data[], item, weights[]) {
    class_min = data[0].class;
    dist_min = eu_dist_with_weigths(item, data[0], weights);
    aux_weights[];

    // Discards weights lower than 0.2
    for w in weights {
        if w < 0.2 {
            aux_weights.push(0.0);
        } else {
            aux_weights.push(w);
        }
    }

    for example in data {
        if example.id != item.id {
            aux_distance = euclidean_distance_with_weigths
                (item, example, aux_weights);
            if aux_distance < d_min {
                class_min = example.class;
                dist_min = aux_distance;
            }
        }
    }

    return c_min;
}

\end{verbatim}

Además hemos definido dos funciones más que toman como argumento un conjunto de elementos de un tipo que implemente \texttt{DataElem}.\\

La primera es \texttt{normalize\_data}, la cual devuelve un conjunto con el mismo número de elementos que el original, con la diferencia de que en este nuevo conjunto todos los elementos tienen sus atributos normalizados respecto al conjunto original. \\

La segunda es \texttt{make\_partitions}, la cual devuelve un vector con 5 conjuntos de elementos que representan 5 particiones, que mantienen en la medida de lo posible la distribución de clases del conjunto original.\\

\subsubsection{Práctica 2}
Para esta práctica hemos definido cuatro operadores, uno de selección, dos de cruce y uno de mutación. Estos se los pasaremos posteriormente a los algoritmos principales, permitiendo en un futuro definir un operador arbitrario y poder ejecutar los algoritmos sin tener que modificar la función principal.

El primer operador que hemos definido es \textbf{binary\_tournament\_selection}, que realiza la selección por torneo binario. Este toma como argumentos el conjunto de \texttt{ChromosomeAndResult} del que eligirá a los posibles padres, el número de padres que queremos generar, y por último el rng para controlar la generación aleatoria.

\begin{verbatim}
binary_tournament_selection(population[], num_parents, rng)  {
    population_size = population.size();
    parents[num_parents];

    for _ in 0..num_parents {
        selector =
            rng.gen_range(0, population_size * population_size);
        first_competitor = population[selector / population_size];
        second_competitor = population[selector % population_size];

        if first_competitor > second_competitor {
            parents.push(first_competitor);
        } else {
            parents.push(second_competitor);
        }
    }

    return parents;
}

\end{verbatim}

El siguiente operador que definimos es \textbf{arithmetic\_mean\_crossover}, que toma como argumentos dos \texttt{Chromosome} y un porcentaje \texttt{alpha}. Inicialmente este operador devolvía un único hijo que era la combinación convexa entre los dos padres al 50\%. Sin embargo, para ayudar a la abstracción del problema lo hemos definido de manera que puedas elegir el porcentaje que mantiene de cada uno de los padres. Así, devuelve dos hijos, uno que posea un porcentaje $\alpha$ de un padre y  un porcentaje $1-\alpha$ de otro y viceversa.

\begin{verbatim}
arithmetic_mean_crossover(one, other, alpha) {
    chromosomes[2];

    for gene in 0..one.size() {
        chromosomes[0]
            .push(one[gene] * (alpha) + other[gene] * (1. - alpha));
        chromosomes[1]
            .push(one[gene] * (1. - alpha) + other[gene] * (alpha));
    }

    return chromosomes;
}

\end{verbatim}

A continuación definimos otro operador de cruce, \textbf{blx\_alpha\_crossover}. Este toma como argumentos dos \texttt{Chromosome} al igual que al anterior, otro porcentaje \texttt{alpha} , y un rng.

\begin{verbatim}
blx_alpha_crossover(one, other, alpha, rng) {
    chromosomes[2];

    for gene in 0..one.size() {
        c_max;
        c_min;

        if one[gene] < other[gene] {
            c_max = other[gene];
            c_min = one[gene];
        } else if other[gene] < one[gene] {
            c_max = one[gene];
            c_min = other[gene];
        } else {
            chromosomes[0].push(one[gene]);
            chromosomes[1].push(one[gene]);

            continue;
        }

        dist = c_max - c_min;

        gene_value1 =
            rng.gen_range(c_min - dist * alpha, c_max + dist * alpha);
        gene_value2 =
            rng.gen_range(c_min - dist * alpha, c_max + dist * alpha);

        // Truncate into [0,1] gene_value1
        if gene_value1 < 0. {
            gene_value1 = 0.;
        } else if gene_value1 > 1. {
            gene_value1 = 1.;
        }

        // Truncate into [0,1] gene_value2
        if gene_value2 < 0. {
            gene_value2 = 0.;
        } else if gene_value2 > 1. {
            gene_value2 = 1.;
        }

        chromosomes[0].push(gene_value1);
        chromosomes[1].push(gene_value2);
    }

    chromosomes
}

\end{verbatim}

% TODO BUSCAR NOMBRE DEL OPERADOR
Por último, el operador de mutación $Mov(\mu, \sigma^2)$ que ya utilizamos en la práctica uno lo hemos definido en una función a parte, con el fin de evitar repiticiones.

\begin{verbatim}
weight_mutation(&weights, index, mean, std_deviation, rng) {
    // Normal distribution with determined mean and standard deviation
    normal = Normal(mean, std_deviation);

    weights[index] += normal.sample(rng);

    // Truncate into [0,1]
    if weights[index] < 0. {
        weights[index] = 0.;
    } else if weights[index] > 1. {
        weights[index] = 1.;
    }
}
\end{verbatim}
\subsection{Función objetivo}
Fijado un problema de clasificación, para evaluar el rendimiento de un vector de pesos calculado sobre un conjunto de datos utilizaremos tres funciones:
\begin{itemize}
    \item \texttt{class\_rate}. A partir de la predicción obtenida mediante el clasificador 1nn utilizando dicho vector de pesos, devuelve porcentaje de elementos bien clasificados(tasa de clasficación):
    \[
    class\_rate = 100 \cdot \frac{n}{N}
    .\]
Donde $n$ es el número de elementos bien clasificados, y $N$ es el número de elementos total en el conjunto
    \item \texttt{red\_rate} A partir del vector de pesos, devuelve el número de pesos menores de $0.2$, que son los que no se tendrán en cuenta en el clasificador(tasa de reducción).
    \[
    red\_rate = 100 \cdot \frac{w}{W}
    .\]
Donde $w$ es el número de pesos con valor menor a $0.2$ y $W$ es el número total de pesos.
    \item \texttt{evaluation\_function} A partir de la tasa de clasificación y la tasa de reducción, devuelve una suma ponderada de ambas. En nuestro caso,$\alpha$ vale $0.5$, luego la ponderación es la misma para ambas tasas.
    \[
    evaluation\_function = \alpha \cdot class\_rate + (1-\alpha)\cdot red\_rate
    .\]
\end{itemize}

\newpage
\section{Descripción de los algoritmos}
\subsection{Práctica 1}
\subsubsection{Algoritmo RELIEF (greedy)}

El algoritmo RELIEF itera sobre todos los elementos del conjunto de datos que recibe, así que de ahora en adelante nos referiremos al elemento actual como \texttt{current\_elem}, y hasta que lo mencionemos de nuevo todo el código estará dentro de dicho bucle.
\begin{verbatim}

weights[] = {0};

for current_elem in data {
    ...
}
\end{verbatim}

En primer lugar calcula el \textit{aliado} y el \textit{enemigo} más cercano para cada elemento utilizando la distancia euclídea usual.
\begin{verbatim}
index = 0;
nearest_enemy_index = 0;
nearest_ally_index = 0;
best_enemy_distance = MAX_VALUE;
best_ally_distance = MAX_VALUE;

for candidate in data {
    if elem.id != candidate.id {
        aux_distance = euclidean_distance
            (current_elem, candidate);

        // Ally search
        if elem.class == candidate.class {
            if aux_distance < best_ally_distance {
                best_ally_distance = aux_distance;
                nearest_ally_index = index;
            }
        }
        // Enemy search
        else {
            if aux_distance < best_enemy_distance {
                best_enemy_distance = aux_distance;
                nearest_enemy_index = index;
            }
        }
    }
}

nearest_ally = data[nearest_ally_index];
nearest_enemy = data[nearest_enemy_index];
\end{verbatim}

A continuación, a cada peso le suma la distancia entre el atributo correspondiente del elemento actual(\texttt{current\_elem}) y de su aliado más cercano. Análogamente le resta la distancia respectiva a su enemigo más cercano.

\begin{verbatim}
for attr in 0 to num_attrs {
    attr_ally_dist =
        (elem.attributes(attr) - nearest_ally.attributes(attr)).abs;
    attr_enemy_dist =
        (elem.attributes(attr) - nearest_enemy.attributes(attr)).abs;

    weights[attr] += attr_enemy_dist - attr_ally_dist;
}
\end{verbatim}

Por último, fuera del bucle principal, truncamos los pesos negativos al intervalo $[0,+\infty]$ y luego normalizamos utilizando el máximo de todos los pesos.
\begin{verbatim}
max_weight = weights[0];

for w in weights {
    if w > max_weight
        max_weight = w;
}

for w in weights {
    if w < 0.0 {
        w = 0.0;
    } else {
        w = w / max_weight;
    }
}

return weights;
\end{verbatim}

\subsubsection{Algoritmo de búsqueda local}
En primer lugar, inicializamos todas las variables necesarias, y clasificamos el conjunto de datos utilizando un conjunto de pesos generados aleatoriamente.
\begin{verbatim}
local_search(data[], rng) {

    indexes[];
    // Nos referiremos a estas dos lineas como fill_and_refresh_indexes()
    indexes = {0, ..., num_attrs};
    indexes.shuffle(rng);

    // Local search parameters
    num_of_mutations = 0;
    max_mutations = 15000;
    neighbours_without_mutting = 0;
    max_neighbour_without_muting = 20 * num_attrs;

    normal = Normal(0.0, 0.3);
    uniform = Uniform(0.0, 1.0);

    // Initialize random weights (using normal distribution)
    weights[];
    for ind in 0 to num_attrs {
        weights.push(uniform.sample(rng));
    }

    initial_guessing[];
    // Initialize current guessing
    for elem in data {
        initial_guessing.push(
            classifier_1nn_with_weights(data, elem, weights));
    }
    current_ev_rate = evaluation_function(
        class_rate(data, initial_guessing),
        red_rate(weights),
        0.5,
    );
\end{verbatim}

A continuación, en el bucle principal seleccionamos un índice aleatorio, lo mutamos, y comprobamos si el vector de pesos con la mutación mejora al actual, en cuyo caso actualizamos este y continuamos ejecutando

\begin{verbatim}

    while neighbours_without_mutting < max_neighbour_without_muting
        && num_of_mutations < max_mutations
    {
        aux_weights = weights;

        fill_and_refresh_indexes();
        index = indexes.pop();

        // Mutation
        aux_weights[index] += normal.sample(rng);

        // Truncate into [0,1]
        aux_weights[index] = truncate(0, 1, aux_weights[index]);

        aux_guessing[];
        // Initialize candidate guessing
        for elem in data {
            aux_guessing.push(
                classifier_1nn_with_weights(data, elem, aux_weights));
        }
        aux_ev_rate = evaluation_function(
            class_rate(data, aux_guessing),
            red_rate(aux_weights),
            0.5,
        );

        if aux_ev_rate > current_ev_rate {
            current_ev_rate = aux_ev_rate;
            weights = aux_weights;
            neighbours_without_mutting = 0;
            fill_and_refresh_indexes();
        } else
            neighbours_without_mutting += 1;

        num_of_mutations += 1;
    }

    return weights;
}

\end{verbatim}

\subsection{Práctica 2}
\subsubsection{Algoritmo Genético Generacional}
Los argumentos que acepta esta función son:
\begin{itemize}
    \item \textbf{data} Conjunto sobre el que evaluaremos los pesos.
    \item \textbf{rng} Generador de números aleatorios.
    \item \textbf{population\_size} Tamaño de la población durante la ejecución del algoritmo.
    \item \textbf{crossover\_probability} Probabilidad de cruce.
    \item \textbf{mutation\_probability} Probabilidad de mutación.
    \item \textbf{max\_calls\_to\_eval} Número maximo de llamadas a la función de evaluación
    \item \textbf{selection\_operator} Operador de selección. Debe aceptar como argumentos un vector de \texttt{ChromosomeAndResult}, un entero, un rng y devolver un vector de \texttt{ChromosomeAndResult}.
    \item \textbf{crossover\_operator} Operador de cruce. Debe aceptar como argumentos dos \texttt{Chromosome}, un rng y devolver dos \texttt{Chromosome}.
    \item \textbf{mutation\_operator} Operador de mutación. Debe aceptar como argumentos una referencia a un \texttt{Chromosome}, un entero y un rng.
\end{itemize}
\begin{verbatim}
generational_genetic_algorithm(
    data,
    rng,
    population_size,
    crossover_probability,
    mutation_probability,
    max_calls_to_eval,
    selection_operator(ChromosomeAndResult[], int, StdRng)
        -> ChromosomeAndResult[],
    crossover_operator(Chromosome, Chromosome, StdRng)
        -> Chromosome[2],
    mutation_operator(&Chromosome, int, StdRng)
){
    quick_eval = |weights: &Chromosome| {
        return evaluate(data, data, weights, 0.5).2;
    };
\end{verbatim}

En primer lugar creamos una función lambda que para evaluar de manera cómoda un vector de pesos.
A continuación inicializamos todas las variables, y rellenamos la población de cromosomas generados aleatoriamente. Tras esto, ordenamos la población y almacenamos el mejor cromosoma.

\begin{verbatim}
    generation_counter = 0;

    num_gens_per_chromosome = get_num_attributes();
    uniform = Uniform(0.0, 1.0);
    calls_to_eval = 0;

    // Initialization ----
    current_population[];

    aux_weights[num_gens_per_chromosome];
    // Initialize random weights (using uniform distribution)
    for _ in 0..population_size {
        for _ in 0..num_gens_per_chromosome {
            aux_weights.push(uniform.sample(rng));
        }

        aux_chrom_and_result = ChromosomeAndResult {
            chromosome: aux_weights,
            result: quick_eval(aux_weights),
        };

        calls_to_eval += 1;

        current_population.push(aux_chrom_and_result);

        aux_weights.clear();
    }

    current_population.sort();
    current_best_chrom_res = current_population.last();
\end{verbatim}

A continuación, entramos en el bucle principal del que no saldremos hasta que no agotemos el número de llamadas a la función de evaluación. \\

Aplicamos el operador de selección que nos pasan como argumento y generamos tantos padres como tamaño tenga nuestra población. Ahora, como el operador de cruce siempre nos devuelve dos hijos, cruzamos el número de padres esperado (calculado a partir de la probabilidad de cruce), y añadimos los hijos a la población auxiliar (los añadimos con resultado $-1$ para calcularlo más tarde). Tras esto, añadimos a la población auxiliar el resto de padres hasta alcanzar el tamaño de la población.

\begin{verbatim}
while calls_to_eval < max_calls_to_eval {

    auxiliar_population[population_size];

    // Selection  ----
    parents =
        selection_operator(current_population, population_size, rng);

    // Crossover (arithmetic mean) ----
    num_of_crossovers =
        (crossover_probability * population_size ) / 2;

    // Adds children chromosomes
    for index in 0..num_of_crossovers {
        children = crossover_operator(
            parents[2 * index].chromosome,
            parents[2 * index + 1].chromosome,
            rng,
        );

        auxiliar_population.push(
            ChromosomeAndResult.new(children[0], -1.));
        auxiliar_population.push(
            ChromosomeAndResult.new(children[1], -1.));
    }

    // Adds remaining chromosomes from parents
    for index in 2 * num_of_crossovers..population_size {
        auxiliar_population.push(parents[index]);
    }
\end{verbatim}

Ahora, para calcular el número de mutaciones a realizar, calculamos el número de mutaciones esperadas sin perder los decimales. Con esto, el número de mutaciones a realizar será la parte de real de dicha esperanza. Para ser más precisos con el número de mutaciones, generaremos un número aleatorio entre $0$ y $1$. Si este es más pequeño que la parte real de la esperanza, sumaremos uno al número de mutaciones a realizar.

\begin{verbatim}
// Mutation ----
expected_num_of_mutations =
    mutation_probability * population_size  * num_gens_per_chromosome;
num_of_mutations = expected_num_of_mutations as usize;

if rng.gen_range(0, 1) <
    (expected_num_of_mutations - num_of_mutations) {
    num_of_mutations += 1;
}

for _ in 0..num_of_mutations {
    selector =
        rng.gen_range(0, population_size * num_gens_per_chromosome);
    chosen_chromosome =
        auxiliar_population[selector % population_size].chromosome;
    chosen_gene = selector / population_size;

    // Mutation operator
    mutation_operator(chosen_chromosome, chosen_gene, rng);

    auxiliar_population[selector % population_size].result =
        quick_eval(chosen_chromosome);

    calls_to_eval += 1;
}

\end{verbatim}

Por último evaluamos todos aquellos cromosomas que no estuviesen ya evaluados, ordenamos la población auxiliar y almacenamos el mejor cromosoma de la población auxiliar. Tras esto, comparamos los dos mejores cromosomas y nos quedamos con el mejor de los dos para la nueva población. Actualizamos toda la población y continuamos.

\begin{verbatim}
    // Evaluation ----
        for chrom_and_res in auxiliar_population {
            if chrom_and_res.result == -1. {
                chrom_and_res.result =
                    quick_eval(chrom_and_res.chromosome);
                calls_to_eval += 1;
            }
        }

        auxiliar_population.sort();
        new_best_chrom_res = auxiliar_population.last();

        // Replacement ----
        // If new best chrom is worst than the old one
        if new_best_chrom_res < current_best_chrom_res {
            auxiliar_population.remove(0);
            auxiliar_population.push(current_best_chrom_res);
        } else {
            current_best_chrom_res = new_best_chrom_res;
        }

        current_population = auxiliar_population;

        generation_counter += 1;
    }

    return current_best_chrom_res.chromosome;
}
\end{verbatim}

\subsubsection{Algoritmo Genético Estacionario}
Para evitar repeticiones, de este algoritmo únicamente hablaremos de las diferencias con el generacional.
\begin{verbatim}
steady_state_genetic_algorithm(
    data,
    rng,
    population_size,
    crossover_probability,
    mutation_probability,
    max_calls_to_eval,
    selection_operator(ChromosomeAndResult[], int, StdRng)
        -> ChromosomeAndResult[],
    crossover_operator(Chromosome, Chromosome, StdRng)
        -> Chromosome[2],
    mutation_operator(&Chromosome, int, StdRng)
){
    quick_eval = |weights: &Chromosome| {
        return evaluate(data, data, weights, 0.5).2;
    };

    generation_counter = 0;

    num_gens_per_chromosome = get_num_attributes();
    uniform = Uniform(0.0, 1.0);
    calls_to_eval = 0;

    // Initialization ----
    current_population[population_size];

    aux_weights[num_gens_per_chromosome];
    // Initialize random weights (using normal distribution)
    for _ in 0..population_size {
        for _ in 0..num_gens_per_chromosome {
            aux_weights.push(uniform.sample(rng));
        }

        aux_chrom_and_result = ChromosomeAndResult {
            chromosome: aux_weights,
            result: quick_eval(aux_weights),
        };

        calls_to_eval += 1;

        current_population.push(aux_chrom_and_result);

        aux_weights.clear();
    }

    current_population.sort();
    current_best_chrom_res = current_population.last();

    while calls_to_eval < max_calls_to_eval {
\end{verbatim}
En este algoritmo únicamente necesitaremos dos hijos, así que solo generamos dos padres.
\begin{verbatim}
// Selection  ----
parents = selection_operator(current_population, 2, rng);

// Crossover  ----
children =
    crossover_operator(
        parents[0].chromosome,
        parents[1].chromosome, rng
    );
auxiliar_population[2];
auxiliar_population.push(
    ChromosomeAndResult.new(children[0], -1));
auxiliar_population.push(
    ChromosomeAndResult.new(children[1], -1));

// Mutation ----
expected_num_of_mutations =
    mutation_probability * population_size  * num_gens_per_chromosome ;
num_of_mutations = expected_num_of_mutations as usize;

if rng.gen_range(0, 1) <
    (expected_num_of_mutations - num_of_mutations) {
    num_of_mutations += 1;
}

for _ in 0..num_of_mutations {
    selector =
        rng.gen_range(0, population_size * num_gens_per_chromosome);
    chosen_chromosome =
        auxiliar_population[selector % population_size].chromosome;
    chosen_gene = selector / population_size;

    // Mutation operator
    mutation_operator(chosen_chromosome, chosen_gene, rng);

    auxiliar_population[selector % population_size].result =
        quick_eval(chosen_chromosome);

    calls_to_eval += 1;
}

for chrom_and_res in auxiliar_population.iter_mut() {
    if chrom_and_res.result == -1. {
        chrom_and_res.result = quick_eval(chrom_and_res.chromosome);
        calls_to_eval += 1;
    }
}

        auxiliar_population.sort();
\end{verbatim}
Ahora, al reemplazar, comparamos los dos hijos que hemos obtenido con los peores cromosomas de nuestra población actual, y de entre estos cuatro nos quedamos con los dos mejores.
\begin{verbatim}
        // Replacement ----
        // Keeps two best of
        // current_population[0], current_population[1],
        // auxiliar_population[0], auxiliar_population[1]

        // if previous best is worse than new worst replace
        // both previous
        if current_population[1] < auxiliar_population[0] {
            current_population.remove(0);
            current_population.remove(1);
            current_population.push(auxiliar_population[0]);
            current_population.push(auxiliar_population[1]);
        }
        // if not, if previous worst is worse than new best
        // replace only him
        else if current_population[0] < auxiliar_population[1] {
            current_population.remove(0);
            current_population.push(auxiliar_population[1]);
        }

        current_population.sort();
        current_best_chrom_res = current_population.last();

        generation_counter += 1;
    }

    return current_best_chrom_res.chromosome;
}

\end{verbatim}

\subsubsection{Búsqueda local de baja intensidad}
Para el siguiente algoritmo hemos tenido que implementar una búsqueda local modificada, la cual únicamente realiza $2\cdot número\_de\_pesos$ iteraciones y funciona prácticamente igual que  la descrita anteriormente. Esta toma como argumentos el conjunto de datos, una referencia a un cromosoma junto a su evaluación, siendo su vector de pesos el inicial y un rng. La salida es el número de llamadas realizadas a la función inicial, y almacena en el vector que le pasamos como argumento el resultado de la búsqueda.

\begin{verbatim}
low_intensity_local_search(data[], weights_and_result[], rng){
    num_attrs = get_num_attributes();
    mut calls_to_eval = 0;

    // Closure that fills and shuffle indices
    fill_and_shuffle = |indices: &mut Vec<usize>, rng: &mut StdRng|{
        *indices = (0..num_attrs).collect();
        indices.shuffle(rng);
    };

    // Initialize vector of indices and shuffles it
    indices[num_attrs];
    fill_and_shuffle(&indices, rng);

    // Normal distribution with mean = 0.0, standard deviation = 0.3
    let normal = Normal(0.0, 0.3);

    for _ in 0..2 * num_attrs {
        aux_weights = weights_and_result.chromosome;

        index = indices.pop();

        // Mutation
        aux_weights[index] += normal.sample(rng) as f32;

        // Truncate into [0,1]
        if aux_weights[index] < 0. {
            aux_weights[index] = 0.;
        } else if aux_weights[index] > 1. {
            aux_weights[index] = 1.;
        }

        aux_ev_rate = evaluate(data, data, &aux_weights, 0.5).2;
        calls_to_eval += 1;

        if aux_ev_rate > weights_and_result.result {
            weights_and_result.result = aux_ev_rate;
            weights_and_result.chromosome = aux_weights;
            fill_and_shuffle(&indices, rng);
        } else {
            if indices.is_empty() {
                fill_and_shuffle(&indices, rng);
            }
        }
    }

    return calls_to_eval;
}

\end{verbatim}

\subsubsection{Algoritmo Memético}
Este funciona exactamente igual que el algoritmo generacional descrito anteriormente, con la diferencia de que cada 10 generaciones realiza una búsqueda local de baja intensidad sobre cierto subconjunto de la población. Este depende del parámetro \texttt{memetic\_type}, que puede valer $1$, $2$, o $3$ respectivamente.
\begin{verbatim}
generational_memetic_algorithm(
    data,
    rng,
    population_size,
    crossover_probability,
    mutation_probability,
    max_calls_to_eval,
    memetic_type,
    selection_operator(ChromosomeAndResult[], int, StdRng)
        -> ChromosomeAndResult[],
    crossover_operator(Chromosome, Chromosome, StdRng)
        -> Chromosome[2],
    mutation_operator(&Chromosome, int, StdRng)
){
    quick_eval = |weights: &Chromosome| {
        return evaluate(data, data, weights, 0.5).2;
    };

    generation_counter = 0;

    num_gens_per_chromosome = get_num_attributes();
    uniform = Uniform(0.0, 1.0);
    calls_to_eval = 0;

    // Initialization ----
    current_population[];

    aux_weights[num_gens_per_chromosome];
    // Initialize random weights (using uniform distribution)
    for _ in 0..population_size {
        for _ in 0..num_gens_per_chromosome {
            aux_weights.push(uniform.sample(rng));
        }

        aux_chrom_and_result = ChromosomeAndResult {
            chromosome: aux_weights,
            result: quick_eval(aux_weights),
        };

        calls_to_eval += 1;

        current_population.push(aux_chrom_and_result);

        aux_weights.clear();
    }

    current_population.sort();
    current_best_chrom_res = current_population.last();

    while calls_to_eval < max_calls_to_eval {

        auxiliar_population[population_size];

        // Selection  ----
        parents =
            selection_operator(
                current_population,
                population_size,
                rng
            );

        // Crossover (arithmetic mean) ----
        num_of_crossovers =
            (crossover_probability * population_size ) / 2;

        // Adds children chromosomes
        for index in 0..num_of_crossovers {
            children = crossover_operator(
                parents[2 * index].chromosome,
                parents[2 * index + 1].chromosome,
                rng,
            );

            auxiliar_population.push(
                ChromosomeAndResult.new(children[0], -1.));
            auxiliar_population.push(
                ChromosomeAndResult.new(children[1], -1.));
        }

        // Adds remaining chromosomes from parents
        for index in 2 * num_of_crossovers..population_size {
            auxiliar_population.push(parents[index]);
        }

        // Mutation ----
        expected_num_of_mutations =
            mutation_probability *
            population_size  *
            num_gens_per_chromosome ;

        num_of_mutations = expected_num_of_mutations as usize;

        if rng.gen_range(0, 1) <
            (expected_num_of_mutations - num_of_mutations) {
            num_of_mutations += 1;
        }

        for _ in 0..num_of_mutations {
            selector =
                rng.gen_range(
                    0,
                    population_size * num_gens_per_chromosome);

            chosen_chromosome =
                auxiliar_population[selector % population_size]
                    .chromosome;
            chosen_gene = selector / population_size;

            // Mutation operator
            mutation_operator(chosen_chromosome, chosen_gene, rng);

            auxiliar_population[selector % population_size].result =
                quick_eval(chosen_chromosome);

            calls_to_eval += 1;
        }

        // Evaluation ----
        for chrom_and_res in auxiliar_population.iter_mut() {
            if chrom_and_res.result == -1. {
                chrom_and_res.result =
                    quick_eval(&chrom_and_res.chromosome);
                calls_to_eval += 1;
            }
        }

        auxiliar_population.sort();
        new_best_chrom_res = auxiliar_population.last();

        // Replacement ----
        // If new best chrom is worst than the old one
        if new_best_chrom_res < current_best_chrom_res {
            auxiliar_population.remove(0);
            auxiliar_population.push(current_best_chrom_res.clone());
        } else {
            current_best_chrom_res = new_best_chrom_res;
        }

        current_population = auxiliar_population;

        generation_counter += 1;

\end{verbatim}

Hasta aquí el algoritmo es igual que el generacional como ya dijimos. Aquí, cada 10 generaciones aplicamos la búsqueda local de baja intensidad. Según los posibles valores de \texttt{memetic\_type} la aplicamos sobre distintos subconjuntos.
\begin{itemize}
    \item \textbf{1.} Aplicamos la búsqueda local de baja intensidad sobre cada cromosoma de nuestra población
    \item \textbf{2.} La aplicamos sobre un $10\%$ aleatorio de la población (calculamos el número de elementos sobre los que aplicarla de la misma forma que lo hicimos para el número de mutaciones)
    \item \textbf{3.} La aplicamos sobre un $10\%$ de la población, pero en lugar de elegirlo de manera aleatoria elegimos a los mejores de la población
\end{itemize}

\begin{verbatim}
if generation_counter % 10 == 0 {
    match memetic_type {
        // Apply local search to every chromosome
        1 => {
            for chrom_and_res in current_population {
                calls_to_eval +=
                    low_intensity_local_search(
                        data,
                        chrom_and_res,
                        rng
                    );
            }
        }

        // Apply to a random percentage of the population
        2 => {
            local_seach_probability = 0.1;
            expected_local_searches =
                local_seach_probability * population_size;
            num_local_searches = expected_local_searches as usize;

            if rng.gen_range(0., 1.) <
                expected_local_searches - num_local_searches {
                num_local_searches += 1;
            }

            for _ in 0..num_local_searches {
                index = rng.gen_range(0, population_size);
                calls_to_eval +=
                    low_intensity_local_search(
                        data,
                        current_population[index],
                        rng
                    );
            }
        }
        3 => {
            num_local_searches = (0.1 * population_size) as usize;

            for ind in 0..num_local_searches {
                calls_to_eval += low_intensity_local_search(
                    data,
                    current_population[population_size - ind - 1],
                    rng,
                );
            }
        }
    }
\end{verbatim}

Por último, reordenamos la población y actualizamos el mejor cromosomoma.

\begin{verbatim}
            current_population.sort();
            current_best_chrom_res = current_population.last();
        }
    }

    return current_best_chrom_res.chromosome;
}
\end{verbatim}

\newpage
\section{Procedimiento considerado para desarrollar la práctica.}
La implementación de la práctica se ha llevado a cabo en el lenguaje de programación Rust, y no se ha utilizado ningún framework de metaheurísticas.\\

El código se encuentra en \texttt{src/main.rs}. Para compilarlo, en primer lugar es necesario instalar rustup, lo cual se encargará de instalar el propio lenguaje y la herramienta Cargo. Para instalar rustup solo es necesario ejecutar este comando:
\begin{shell}
curl https://sh.rustup.rs -sSf | sh
\end{shell}

Tras esto, para compilar el programa y ejecutarlo tenemos que hacer:
\begin{shell}
cargo build --release
cargo run --release <args>
\end{shell}

El único argumento que acepta el programa es un entero, que será el que actuará como semilla para cualquier valor aleatorio sea necesario en el programa. Si se ejecuta sin argumentos, se toma como semilla por defecto el valor $1$.

El programa ejecuta todos los algoritmos, y muestra por la salida estándar en formato tabla los resultados de cada algoritmo para los 3 conjuntos de datos proporcionados en la carpeta \texttt{data}.

\newpage

\section{Experimentos y análisis de los resultados}
\subsection{Descripción de los casos del problema}
\subsubsection{Colposcopy}
Colposcopy es una base de datos sobre un procedimiento ginecológico que consiste en la exploración del cuello uterino. El fichero proporcionado contiene $287$ elementos que representan una imagen de la secuencia colposcópica.\\

Cada ejemplo consta de 62 atributos reales(representan características extraidas de las imágenes) y una clase que puede ser de dos tipos, positivo o negativo.

\subsubsection{Ionosphere}
Ionosphere es una base de datos de radar recogidos por un sistema en Goose Bay, Labrador. Este sistema consiste en un conjunto de fases de 16 antenas de alta frecuencia con una potencia total transmitida del orden de 6,4 kilovatios. El fichero proporcionado contiene $352$ elementos.

Cada ejemplo consta de 34 atributos reales y de una clase que puede ser bueno (si el retorno de radar muestra evidencia de algún tipo de estructura en la ionosfera) o malo (las señales simplemente atraviesan la ionosfera).

\subsubsection{Texture}
Texture es una base de datos que contiene datos sobre fotografias de distintos tipos de texturas con el objetivo de distinguir entre 11 texturas diferentes (césped, piel de becerro prensada, papel hecho a mano, rafia en bucle a una pila alta, lienzo de algodón, \dots). El fichero proporcionado contiene $550$ ejemplos.

Cada ejemplo consta de 40 atributos construidos mediante la estimación de momentos modificados de cuarto orden en cuatro orientaciones: 0, 45, 90 y 135 grados, y de una clase que indica el tipo de textura.

\subsection{Resultados obtenidos}
En esta sección mostraremos los resultados obtenidos al ejecutar cada uno de los algoritmos sobre cada conjunto de datos proporcionado. Mostraremos los resultados en tablas, una por cada algoritmo y conjunto de datos. Cada tabla posee 5 columnas:
\begin{itemize}
    \item \textbf{Partición}. Indica que partición de las 5 se está utilizando como conjunto de prueba.
    \item \textbf{Tasa\_class}. Tasa de clasificación obtenida.
    \item \textbf{Tasa\_red}. Tasa de reducción obtenida.
    \item \textbf{Agregado}. Función de evaluación utilizando los valores anteriores.
    \item \textbf{Tiempo}. Tiempo de ejecución del algoritmo en milisegundos.
\end{itemize}

La ejecución que mostraremos se ha realizado utilizando como semilla el valor por defecto $1$. La ejecución ha sido realizado en mi portatil personal con sistema operativo Manjaro Linux 18.0.3, y procesador Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\\

\subsubsection{Práctica 1}
En las primeras seis tablas (1-6) se muestran los resultados de dos algoritmos aleatorios distintos: el primero elige una clase aleatoria de entre las conocidas para clasificar y el segundo crea un vector de pesos aleatorio y clasifica de manera normal utilizándolo

En las siguientes tres tablas (7-9) se muestran los resultados obtenidos utilizando simplemente el algoritmo \textbf{1nn} para clasificar. A continuación, se muestran los resultados asociados al algoritmo greedy \textbf{RELIEF} (tablas 10-12). Por último, las tres tablas restantes (13-15) muestran los resultados obtenidos utilizando la \textbf{búsqueda local} como algoritmo para resolver el problema de APC.

\subsubsection{Pŕactica 2}
En primer lugar, mencionemos las constantes que hemos utilizado para ejecutar los algoritmos:
\begin{itemize}
    \item El \textbf{tamaño de la población} ha sido $30$ para los algoritmos genéticos, y $10$ para los algoritmos meméticos.
    \item El número \textbf{máximo de llamadas a la función de evaluación} ha sido 15000 en todos los algoritmos.
    \item En los algoritmos generacionales (tanto los genéticos como los meméticos) la \textbf{probabilidad de cruce} ha sido $0.7$ mientras que en los estacionarios es 1 (debido a que únicamente realiza un cruce en cada iteración).
    \item La \textbf{probabilidad de mutación} ha sido $0.001$ en todos los algoritmos.
    \item El operador de selección es siempre el \textbf{torneo binario}.
    \item El parámetro utilizado para el \textbf{operador de cruce aritmético} ha sido $0.4$ mientras que para el \textbf{operador de cruce blx} ha sido $0.3$.
    \item Por último, el \textbf{operador de mutación} que hemos utilizado ha sido el de la práctica anterior, con media $0$ y desviación típica  $0.3$.
\end{itemize}

Dicho esto, en las tablas a continuación hemos recogido los resultados de la ejecución de siete algoritmos:
\begin{itemize}
    \item Algoritmo genético generacional con cruce aritmético (\textbf{AGG-Arithmetic}).
    \item Algoritmo genético generacional con cruce blx (\textbf{AGG-BLX}).
    \item Algoritmo genético estacionario con cruce aritmético (\textbf{AGE-Arithmetic}).
    \item Algoritmo genético estacionario con cruce blx (\textbf{AGE-BLX}).
    \item Algoritmo memético generacional con cruce blx sobre toda la población (\textbf{AM-1}).
    \item Algoritmo memético generacional con cruce blx sobre un $10\%$ de la población (\textbf{AM-0.1}).
    \item Algoritmo memético generacional con cruce blx sobre el mejor $10\%$ de la población (\textbf{AM-0.1best}).

\end{itemize}

\newpage
\subsubsection{Tablas Selección Aleatoria}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 64.40678  & 100        & 82.20339  & 0      \\
1         & 59.649124 & 100        & 79.824562 & 0      \\
2         & 70.17544  & 100        & 85.08772  & 0      \\
3         & 61.403507 & 100        & 80.701754 & 0      \\
4         & 57.894737 & 100        & 78.947369 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 50.704224 & 100        & 75.352112 & 0      \\
1         & 45.714287 & 100        & 72.857143 & 0      \\
2         & 45.714287 & 100        & 72.857143 & 0      \\
3         & 60        & 100        & 80.0      & 0      \\
4         & 61.42857  & 100        & 80.714285 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 9.090909  & 100        & 54.5454545 & 0      \\
1         & 7.2727275 & 100        & 53.636364  & 0      \\
2         & 6.3636365 & 100        & 53.1818182 & 0      \\
3         & 11.818182 & 100        & 55.909091  & 0      \\
4         & 9.090909  & 100        & 54.5454545 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas Pesos Aleatorios}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 76.27119  & 22.580645  & 49.425915 & 0      \\
1         & 70.17544  & 20.967741  & 45.57159  & 0      \\
2         & 70.17544  & 14.5161295 & 42.345783 & 0      \\
3         & 71.929825 & 29.032259  & 50.48104  & 0      \\
4         & 77.192986 & 17.741936  & 47.46746  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 17.647058 & 53.89395  & 0      \\
1         & 81.42857  & 26.470589 & 53.94958  & 0      \\
2         & 82.85714  & 17.647058 & 50.252098 & 0      \\
3         & 94.28571  & 26.470589 & 60.37815  & 0      \\
4         & 87.14286  & 17.647058 & 52.39496  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 95.454544 & 10       & 52.727272 & 1      \\
1         & 90        & 25       & 57.5      & 1      \\
2         & 96.36364  & 27.5     & 61.93182  & 1      \\
3         & 90        & 20       & 55        & 1      \\
4         & 93.63636  & 20       & 56.81818  & 1      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Texture}
  \end{table}%

\newpage
\subsubsection{Tablas 1nn}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 74.57627  & 0        & 37.288136 & 0      \\
1         & 70.17544  & 0        & 35.08772  & 0      \\
2         & 73.68421  & 0        & 36.842106 & 0      \\
3         & 75.4386   & 0        & 37.7193   & 0      \\
4         & 82.45614  & 0        & 41.22807  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 0        & 45.070423 & 0      \\
1         & 80        & 0        & 40        & 0      \\
2         & 82.85714  & 0        & 41.42857  & 0      \\
3         & 92.85714  & 0        & 46.42857  & 0      \\
4         & 87.14286  & 0        & 43.57143  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 93.63636  & 0        & 46.81818  & 1      \\
1         & 89.09091  & 0        & 44.545456 & 1      \\
2         & 94.545456 & 0        & 47.272728 & 1      \\
3         & 92.72727  & 0        & 46.363636 & 1      \\
4         & 92.72727  & 0        & 46.363636 & 1      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Texture}
  \end{table}%

\newpage
\subsubsection{Tablas RELIEF}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 72.881355 & 40.322582 & 56.601967 & 3      \\
1         & 75.4386   & 27.419355 & 51.428978 & 3      \\
2         & 77.192986 & 32.258064 & 54.725525 & 3      \\
3         & 71.929825 & 51.612904 & 61.771362 & 3      \\
4         & 82.45614  & 30.64516  & 56.55065  & 3      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 2.9411764 & 46.54101  & 2      \\
1         & 81.42857  & 2.9411764 & 42.184875 & 2      \\
2         & 82.85714  & 2.9411764 & 42.89916  & 2      \\
3         & 92.85714  & 2.9411764 & 47.89916  & 2      \\
4         & 90        & 2.9411764 & 46.47059  & 2      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 91.818184 & 15       & 53.409092 & 7      \\
1         & 90.90909  & 2.5      & 46.704544 & 7      \\
2         & 95.454544 & 2.5      & 48.977272 & 7      \\
3         & 92.72727  & 2.5      & 47.613636 & 7      \\
4         & 93.63636  & 5        & 49.31818  & 7      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Texture}

  \end{table}%

\newpage

\subsubsection{Tablas Búsqueda Local}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 72.881355 & 85.48387  & 79.18262  & 11806  \\
1         & 71.929825 & 80.645164 & 76.28749  & 6312   \\
2         & 68.42105  & 79.03226  & 73.726654 & 8371   \\
3         & 68.42105  & 80.645164 & 74.53311  & 15446  \\
4         & 73.68421  & 77.41936  & 75.55179  & 13647  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{búsqueda local} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 91.54929  & 88.23529 & 89.89229  & 3995   \\
1         & 85.71429  & 85.29412 & 85.5042   & 2753   \\
2         & 84.28571  & 91.17647 & 87.731094 & 4425   \\
3         & 91.42857  & 85.29412 & 88.36134  & 2889   \\
4         & 84.28571  & 82.35294 & 83.31933  & 2318   \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{búsqueda local} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.90909  & 82.5     & 86.704544 & 17734  \\
1         & 89.09091  & 75       & 82.045456 & 14127  \\
2         & 90.90909  & 85       & 87.954544 & 20305  \\
3         & 88.181816 & 80       & 84.09091  & 8280   \\
4         & 87.27273  & 90       & 88.63637  & 27707  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{búsqueda local} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AGG-Arithmetic ($\alpha$ = 0.4)}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 72.881355 & 64.51613  & 68.698746 & 39763  \\
1         & 75.4386   & 70.96774  & 73.20317  & 40507  \\
2         & 71.929825 & 51.612904 & 61.771362 & 40507  \\
3         & 73.68421  & 54.83871  & 64.26146  & 40498  \\
4         & 77.192986 & 64.51613  & 70.85455  & 40390  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-Arithmetic} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 87.323944 & 76.47059  & 81.89726 & 32188  \\
1         & 85.71429  & 67.64706  & 76.68067 & 32468  \\
2         & 85.71429  & 73.52941  & 79.62185 & 32944  \\
3         & 85.71429  & 70.588234 & 78.15126 & 32490  \\
4         & 91.42857  & 67.64706  & 79.53781 & 32409  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-Arithmetic} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.90909  & 75       & 82.954544 & 89442  \\
1         & 90        & 67.5     & 78.75     & 89442  \\
2         & 91.818184 & 72.5     & 82.15909  & 89720  \\
3         & 89.09091  & 75       & 82.045456 & 89095  \\
4         & 89.09091  & 82.5     & 85.795456 & 88959  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-Arithmetic} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AGG-BLX ($\alpha$ = 0.3)}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 69.49152  & 80.645164 & 75.068344 & 39875  \\
1         & 66.666664 & 70.96774  & 68.8172   & 40847  \\
2         & 80.70175  & 82.258064 & 81.479904 & 41838  \\
3         & 71.929825 & 69.354836 & 70.642334 & 41238  \\
4         & 80.70175  & 79.03226  & 79.867004 & 40502  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-BLX} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 87.323944 & 85.29412  & 86.30904 & 32477  \\
1         & 82.85714  & 88.23529  & 85.54622 & 33026  \\
2         & 88.57143  & 85.29412  & 86.93277 & 32443  \\
3         & 95.71429  & 79.411766 & 87.56303 & 32458  \\
4         & 90        & 82.35294  & 86.17647 & 32389  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-BLX} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90        & 80       & 85        & 89601  \\
1         & 86.36364  & 85       & 85.68182  & 89154  \\
2         & 89.09091  & 80       & 84.545456 & 88862  \\
3         & 87.27273  & 82.5     & 84.88637  & 88999  \\
4         & 90        & 85       & 87.5      & 88999  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGG-BLX} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AGE-Arithmetic($\alpha$ = 0.4)}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 79.66102  & 62.903225 & 71.28212  & 40681  \\
1         & 75.4386   & 69.354836 & 72.39671  & 41862  \\
2         & 68.42105  & 75.80645  & 72.113754 & 40758  \\
3         & 71.929825 & 69.354836 & 70.642334 & 40765  \\
4         & 85.96491  & 59.677418 & 72.82117  & 40777  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-Arithmetic} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 91.54929  & 73.52941  & 82.53935 & 32223  \\
1         & 84.28571  & 64.70588  & 74.4958  & 32542  \\
2         & 85.71429  & 76.47059  & 81.09244 & 32719  \\
3         & 94.28571  & 61.764706 & 78.02521 & 32487  \\
4         & 90        & 70.588234 & 80.29411 & 32819  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-Arithmetic} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 94.545456 & 75       & 84.77273  & 88412  \\
1         & 88.181816 & 70       & 79.09091  & 88637  \\
2         & 90.90909  & 65       & 77.954544 & 88360  \\
3         & 90.90909  & 75       & 82.954544 & 89690  \\
4         & 91.818184 & 70       & 80.90909  & 90119  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-Arithmetic} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AGE-BLX ($\alpha$ = 0.3)}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 76.27119  & 79.03226  & 77.65172  & 39925  \\
1         & 68.42105  & 77.41936  & 72.920204 & 40627  \\
2         & 75.4386   & 79.03226  & 77.23543  & 40856  \\
3         & 68.42105  & 69.354836 & 68.88794  & 40881  \\
4         & 73.68421  & 70.96774  & 72.32597  & 41069  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-BLX} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 87.323944 & 85.29412 & 86.30904  & 32322  \\
1         & 84.28571  & 73.52941 & 78.90756  & 32370  \\
2         & 85.71429  & 76.47059 & 81.09244  & 32358  \\
3         & 95.71429  & 82.35294 & 89.033615 & 32462  \\
4         & 85.71429  & 85.29412 & 85.5042   & 32397  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-BLX} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 86.36364  & 80       & 83.18182  & 90643  \\
1         & 81.818184 & 85       & 83.40909  & 90123  \\
2         & 92.72727  & 80       & 86.36363  & 90486  \\
3         & 91.818184 & 85       & 88.40909  & 91006  \\
4         & 89.09091  & 75       & 82.045456 & 90744  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AGE-BLX} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AM-1}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 69.49152  & 87.09677  & 78.29414  & 41640  \\
1         & 68.42105  & 80.645164 & 74.53311  & 42538  \\
2         & 73.68421  & 82.258064 & 77.97114  & 42426  \\
3         & 78.947365 & 85.48387  & 82.21562  & 42218  \\
4         & 75.4386   & 83.870964 & 79.654785 & 42256  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-1} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 88.73239  & 88.23529  & 88.48384  & 33523  \\
1         & 80        & 91.17647  & 85.588234 & 34114  \\
2         & 72.85714  & 94.117645 & 83.4874   & 33812  \\
3         & 82.85714  & 91.17647  & 87.0168   & 33713  \\
4         & 82.85714  & 88.23529  & 85.54622  & 33724  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-1} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 89.09091  & 85       & 87.045456 & 93409  \\
1         & 88.181816 & 82.5     & 85.34091  & 92224  \\
2         & 90.90909  & 85       & 87.954544 & 92258  \\
3         & 87.27273  & 87.5     & 87.38637  & 95025  \\
4         & 89.09091  & 85       & 87.045456 & 93018  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-1} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AM-0.1}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 81.355934 & 82.258064 & 81.807    & 40257  \\
1         & 68.42105  & 79.03226  & 73.726654 & 40981  \\
2         & 78.947365 & 74.19355  & 76.57046  & 41111  \\
3         & 70.17544  & 85.48387  & 77.82965  & 41137  \\
4         & 82.45614  & 82.258064 & 82.3571   & 41200  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 88.73239  & 88.23529 & 88.48384  & 32413  \\
1         & 84.28571  & 91.17647 & 87.731094 & 32466  \\
2         & 88.57143  & 91.17647 & 89.87395  & 32606  \\
3         & 87.14286  & 85.29412 & 86.21849  & 32659  \\
4         & 90        & 88.23529 & 89.117645 & 32661  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 85.454544 & 87.5     & 86.47727 & 89427  \\
1         & 90        & 85       & 87.5     & 89981  \\
2         & 93.63636  & 87.5     & 90.56818 & 89217  \\
3         & 85.454544 & 85       & 85.22727 & 89169  \\
4         & 85.454544 & 85       & 85.22727 & 89236  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas AM-0.1best}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 67.79661  & 85.48387  & 76.64024  & 40137  \\
1         & 70.17544  & 80.645164 & 75.4103   & 41001  \\
2         & 66.666664 & 82.258064 & 74.462364 & 40961  \\
3         & 78.947365 & 83.870964 & 81.409164 & 41011  \\
4         & 73.68421  & 88.70968  & 81.196945 & 41058  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1best} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 83.098595 & 88.23529 & 85.66695  & 32446  \\
1         & 82.85714  & 88.23529 & 85.54622  & 32684  \\
2         & 82.85714  & 88.23529 & 85.54622  & 33144  \\
3         & 90        & 91.17647 & 90.588234 & 32519  \\
4         & 87.14286  & 88.23529 & 87.68907  & 32890  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1best} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 88.181816 & 87.5     & 87.84091 & 89856  \\
1         & 88.181816 & 87.5     & 87.84091 & 89793  \\
2         & 91.818184 & 87.5     & 89.65909 & 89770  \\
3         & 87.27273  & 87.5     & 87.38637 & 89521  \\
4         & 92.72727  & 85       & 88.86363 & 89674  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{AM-0.1best} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas Agrupadas}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random             & 62.705918 & 100 & 81.352959 & 0\\
 Random weights     & 73.148976 & 20.967742 & 47.058359 & 0\\
 1-NN               & 75.266132 & 0 & 37.633066 & 0\\
 RELIEF             & 75.979781 & 36.451613 & 56.215697 & 3\\
 Local search       & 71.067498 & 80.645164 & 75.856331 & 11073.8\\
     AGG-Arithmetic& 74.225395 & 61.290323 & 67.757859 &  40333 \\
     AGG-BLX& 73.898302 & 76.451613 & 75.174957 &  40860 \\
     AGE-Arithmetic& 76.283081 & 67.419353 & 71.851217 & 40968.6 \\
     AGE-BLX&  72.44722 & 75.161291 & 73.804256 & 40671.6 \\
     AM-1& 73.196549 & 83.870966 & 78.533758 & 42215.6 \\
     AM-0.1& 76.271186 & 80.645162 & 78.458174 & 40937.2 \\
     AM-0.1best& 71.454058 & 84.193548 & 77.823803 & 40833.6 \\

  \bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Colposcopy }
  \end{table}%
% TODO Arreglar tablas
\newpage

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random & 52.712274 & 100 & 76.356137 & 0\\
 Random weights & 87.171026 & 21.176470 & 54.173748 & 0\\
1-NN & 86.599598 & 0 & 43.299799 & 0\\
 RELIEF& 87.45674 & 2.9411764 & 45.198958 & 2\\
 Local search& 87.452714 & 86.470588 & 86.961651 & 3235.6\\
     AGG-Arithmetic& 87.179077 & 71.176471 & 79.177774 & 32499.8 \\
     AGG-BLX& 88.893361 & 84.117647 & 86.505504 & 32558.6 \\
     AGE-Arithmetic&    89.167 & 69.411764 & 79.289382 &  32558 \\
     AGE-BLX& 87.750505 & 80.588236 & 84.169370 & 32381.8 \\
     AM-1& 81.460762 & 90.588233 & 86.024498 & 33777.2 \\
     AM-0.1& 87.746478 & 88.823528 & 88.285003 &  32561 \\
     AM-0.1best& 85.191147 & 88.823526 & 87.007337 & 32736.6 \\

  \bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Ionosphere}
  \end{table}%

\newpage

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random             & 8.7272728 & 100 & 54.363637 & 0\\
 Random weights     & 93.090909 & 20.5 & 56.795454 & 1\\
 1-NN               & 92.545453 & 0 & 46.272727 & 1\\
 RELIEF             & 92.909090 & 5.5 & 49.204545 & 7.2\\
 Local search       & 89.272727 & 82.5 & 85.886364 & 17632.4\\
     AGG-Arithmetic& 90.181819 &     74.5 & 82.340909 & 89331.6 \\
     AGG-BLX& 88.545456 &     82.5 & 85.522728 &  89123 \\
     AGE-Arithmetic& 91.272727 &       71 & 81.136364 & 89043.6 \\
     AGE-BLX& 88.363638 &       81 & 84.681819 & 90600.4 \\
     AM-1& 88.909091 &      85. & 86.954546 & 93186.8 \\
     AM-0.1& 87.999998 &      86. & 86.999999 &  89406 \\
     AM-0.1best& 89.636363 &      87. & 88.318182 & 89722.8 \\
\bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Texture}
  \end{table}%

\newpage
\subsection{Análisis de los resultados}
\subsubsection{Práctica 1}
Comencemos hablando del algoritmo que selecciona una clase de las conocidas de manera aleatoria. Al contrario de lo que cabría esperar, obtiene <<buenos>> resultados tanto sobre el conjunto de datos \textit{Colposcopy}, como sobre \textit{Ionosphere}. Esto es debido a que únicamente hay dos clases distintas, luego la tasa de clasificación está cercana al $50\%$ , y además la tasa de reducción es del $100\%$ ya que no utiliza ningún peso. En consecuencia el agregado se encuentra entre el $70\%$ y el $80\%$. Ahora bien, como en \textit{Texture} hay once clases distintas, la tasa de clasificación es bastante mala pero el agregado sigue estando por encima del $50\%$ gracias a la tasa de reducción.

Si nos fijamos ahora en el algoritmo que utiliza un vector de pesos aleatorios, observamos que tanto en el conjunto \textit{Ionosphere} como en \textit{Texture} obtiene muy buenas tasas de clasificación (cercanas al $90\%$), y bastante buenas en \textit{Colposcopy} (cerca de un $73\%$). Sin embargo, la tasa de reducción no es muy grande en ninguno de los casos, estando siempre en torno al $20\%$. Esto provoca que el agregado esté cerca del $50\%$ en todos los casos.

Dicho esto, fijándonos en la tabla del \textbf{1-NN}, en los tres casos obtenemos resultados muy parecidos a los obtenidos utilizando un vector de pesos aleatorio, sin embargo en este caso como tenemos en cuenta todos los pesos (la distancia euclidia toma todos los pesos a $1$), la tasa de reducción es $0$ siempre, lo que se ve reflejado en el agregado, que es aproximadamente un $10\%$ menor que cuando utilizamos un vector de pesos aleatorio.

En todos los algoritmos anteriores, como prácticamente la única operación que hay que hacer es calcular la distancia entre todos los elementos una vez, y los conjuntos no son muy grandes, los tiempos de ejecución son practicamente despreciables.

Si nos fijamos en los resultados obtenidos por uno de los dos algoritmos principales de esta práctica, el \textbf{RELIEF}, las tasas de clasificación son ligeramente mayores o practicamente iguales que en los dos casos anteriores. Sin tener en cuenta el conjunto colposcopy, obtenemos tasas de reducciones muy pequeñas aunque no nulas, provocando que el agregado sea mayor que el del \textbf{1-NN} aunque menor que el vector de pesos aleatorio. En este algoritmo ya observamos que hemos medido un tiempo, aunque bastante pequeño, significativo ya que es necesario recorrer dos veces el conjunto de elementos (una para calcular los enemigos y aliados y calcular los pesos, y otra para ejecutar \textbf{1-NN}).

Por último, si estudiamos la \textbf{búsqueda local} es facil observar que es la que mejores resultados obtiene si tenemos en cuenta la función de evaluación. Esto es debido a que en la implementación de este algoritmo, se buscan maximos de la función de evaluación, por tanto, aunque las tasas de clasificación sean ligeramente menores que en los tres casos anteriores, se consigue una muy alta tasa de reducción, lo que consigue que el agregado esté cercano al $75\%$ en Colposcopy, y al  $89\%$ en los dos conjuntos restantes. Si hablamos del tiempo, en este algoritmo es considerablemente mayor, lo cual era de esperar, pues en cada iteración del bucle principal se evaluán los pesos tras la mutación, por lo que se recorre el conjunto de elementos un mínimo de veinte veces el número de atributos y un máximo de 15000 veces.

Las conclusiones que obtenemos teniendo en cuenta todos los resultados es que, en la función de evaluación le estamos dando demasiada importancia a la reducción, ya que ambos algoritmos aleatorios obtienen prácticamente las puntuaciones más altas a excepción de la \textbf{búsqueda local}. Esta se comporta como podría esperarse, y es que obtiene puntuaciones muy altas, reduciendo un gran número de pesos.

\subsubsection{Práctica 2}
En primer lugar mencionemos el tiempo que tardan los algoritmos, que en está práctica no tiene mucho que comentar. Eso es porque como en todos la condición de parada es el mismo número de llamadas a la función de evaluación, que es la que consume casi todo el tiempo de CPU, pues el tiempo es casi el mismo en todos los algoritmos (cerca de un minuto y medio).

Comencemos analizando los resultados sobre los algoritmos genéticos, ya que los meméticos se basan en estos. En nuestro caso, tenemos que cuatro algoritmos que difieren únicamente en el operador de cruce y en el reemplazo.

Si observamos las tablas agrupadas, es fácil comprobar que en todos los conjuntos de los que disponemos el operador de \textbf{cruce BLX} funciona mejor que el \textbf{aritmético} como era de esperar (entre un $4\%$ y un  $5\%$ de media). Por un lado, el \textbf{cruce aritmético} es demasiado restrictivo, ya que siempre genera un hijo cuyos genes están en una posición especifica muy cercana al centro entre los respectivos genes de los padres. Por otro lado, el \textbf{cruce BLX} realiza una mayor exploración del espacio, ya cada peso del cromosoma hijo lo genera de manera aleatoria en un intervalo alrededor de los pesos de los padres. Esto en conjunto hace que el \textbf{BLX} en general sea mejor operador de cruce.

A continuación, analicemos cuál de los dos esquemas de reemplazo funciona mejor. En las tablas podemos observar que el reemplazo \textbf{generacional} funciona mejor que el \textbf{estacionario}(cerca de un $2\%$ de diferencia media). Esto es debido a que el reemplazo \textbf{generacional} tiene en cuenta una gran parte de la población (se utiliza el $70\%$ de la población como padres), mientras que en el reemplazo \textbf{estacionario}, la población se modifica muy poco entre dos generaciones (como mucho se realizan dos reemplazos). Además, en el \textbf{generacional} la mutación puede ocurrir sobre cualquier elemento de la población auxiliar al completo, mientras que en el \textbf{estacionario} únicamente se puede produce en uno de los dos hijos. Estos dos hechos provocan que como dijimos antes, el reemplazo \textbf{generacional} funcione mejor que el \textbf{estacionario}.

Dicho esto, de los cuatro algoritmos genéticos el que mejor funciona es el \textbf{AGG-BLX}, asi que es el que hemos utilizado como base para el algoritmo memético.

En los algoritmos \textbf{meméticos} intentamos suplir la falta de explotación de las soluciones que presentan los algoritmos anteriores. Estos, al trabajar con una población de tamaño considerable, reparten las mutaciones entre cada uno de los cromosomas de cada generación, lo que nos lleva a pensar que es una buena idea intentar mejorar por separado los cromosomas de los que disponemos. Para no sobreexplotar dichas soluciones, realizaremos una búsqueda local de baja intensidad sobre un cierto subconjunto de la población, y todo esto cada 10 generaciones (para que la población cambie lo suficiente y no quedarse estancado en los mismos máximos locales).

Antes de analizar los resultados entre los distintos algoritmos meméticos hay que resaltar que en general la diferencia de resultados entre ellos es más bien pequeña, debido posiblemente a los conjuntos de datos y a la semilla utilizada (tras varias pruebas con varias semillas, no había una diferencia notable así que hemos mostrado los resultados con la semilla inicial).

Se puede apreciar que el \textbf{AM-1} es el que peor se comporta en general (a excepción del conjunto de datos \textit{Colposcopy}, posiblemente a causa de lo mencionado en el párrafo anterior). Esto ocurre porque aplica la búsqueda local a la población entera, y probablemente haya mínimos locales a los que converjan varios elementos de la población, luego estaríamos <<gastando>> evaluaciones sin obtener mejoras realmente. Con esto, los algoritmos \textbf{AM-0.1} y \textbf{AM-0.1best} son los que mejores resultados obtienen, siendo la diferencia entre ellos poco significante. Aunque a priori podríamos pensar que el \textbf{AM-0.1best} debería ser el mejor puesto que explota únicamente las mejores soluciones, es probable que en repitadas ocasiones los mejores cromosomas compartan mínimos locales como mencionamos antes, por lo que el factor aleatorio del \textbf{AM-0.1} puede ser favorable y de ahí que obtenga resultados similares.

Por último, para realizar una comparación con la práctica anterior, el único algoritmo que está a la altura de los realizados en esta práctica es la \textbf{búsqueda local}. Este obtiene resultados muy parecidos al mejor de los cuatro genéticos (\textbf{AGG-BLX}). Como dijimos en el análisis de resultados de la práctica anterior, es <<rentable>> explotar una solución buena ya que la reducción tiene mucho peso en la función de evaluación, y es <<relativamente sencillo>> eliminar pesos, lo que favorece a la \textbf{búsqueda local}. A priori sería lógico pensar que el \textbf{AGG-BLX} debería obtener mejores resultados, ya que realiza una exploración del espacio bastante más amplia, y también explota aunque en menor medida las soluciones. Sin embargo, al final los resultados son similares por lo que acabamos de mencionar. Ahora bien, al suplir esta falta de explotación de las soluciones en los algoritmos meméticos, los resultados mejoran consistentemente respecto de la \textbf{búsqueda local}, lo que concuerda con lo estudiado teóricamente en el sentido de que es necesario buscar un equilibrio entre la exploración y la explotación.

\end{document}
