\documentclass[size=a4, parskip=half, titlepage=false, toc=flat, toc=bib, 12pt]{scrartcl}

\setuptoc{toc}{leveldown}

% Ajuste de las líneas y párrafos
\linespread{1.2}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% Español
\usepackage[spanish, es-tabla]{babel}

% Matemáticas
\usepackage{amsmath}
\usepackage{amsthm}

% Fuentes
\usepackage{newpxtext,newpxmath}
\usepackage[scale=.9]{FiraMono}
\usepackage{FiraSans}
\usepackage[T1]{fontenc}

\defaultfontfeatures{Ligatures=TeX,Numbers=Lining}
\usepackage[activate={true,nocompatibility},final,tracking=true,factor=1100,stretch=10,shrink=10]{microtype}
\SetTracking{encoding={*}, shape=sc}{0}

\usepackage{graphicx}
\usepackage{float}

% Mejores tablas
\usepackage{booktabs}

\usepackage{adjustbox}
% COLORES

\usepackage{xcolor}

\definecolor{verde}{HTML}{007D51}
\definecolor{esmeralda}{HTML}{045D56}
\definecolor{salmon}{HTML}{FF6859}
\definecolor{amarillo}{HTML}{FFAC12}
\definecolor{morado}{HTML}{A932FF}
\definecolor{azul}{HTML}{0082FB}
\definecolor{error}{HTML}{b00020}

% ENTORNOS
\usepackage[skins, listings, theorems]{tcolorbox}

\newtcolorbox{recuerda}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=black!10,
	lefttitle=0pt,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=black,
  title=\raisebox{-0.6ex}{\small RECUERDA}
}

\newtcolorbox{nota}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=black!10,
	lefttitle=0pt,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=black,
  title=\raisebox{-0.6ex}{\small NOTA}
}

\newtcolorbox{error}{
  enhanced,
%  sharp corners,
  frame hidden,
  colback=error!10,
	lefttitle=0pt,
  coltitle=error,
  fonttitle=\bfseries\sffamily\scshape,
  titlerule=0.8mm,
  titlerule style=error,
  title=\raisebox{-0.6ex}{\small ERROR}
}

\newtcblisting{shell}{
  enhanced,
  colback=black!10,
  colupper=black,
  frame hidden,
  opacityback=0,
  coltitle=black,
  fonttitle=\bfseries\sffamily\scshape,
  %titlerule=0.8mm,
  %titlerule style=black,
  %title=Consola,
  listing only,
  listing options={
    style=tcblatex,
    language=sh,
    breaklines=true,
    postbreak=\mbox{\textcolor{black}{$\hookrightarrow$}\space},
    emph={guille@guille-pc},
    emphstyle={\bfseries},
  },
}

\newtcbtheorem[number within=section]{teor}{\small TEOREMA}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{teor}

\newtcbtheorem[number within=section]{prop}{\small PROPOSICIÓN}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{prop}

\newtcbtheorem[number within=section]{cor}{\small COROLARIO}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{cor}

\newtcbtheorem[number within=section]{defi}{\small DEFINICIÓN}{
  enhanced,
  sharp corners,
  frame hidden,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape
}{defi}

\newtcbtheorem{ejer}{\small EJERCICIO}{
  enhanced,
  sharp corners,
  frame hidden,
  left=0mm,
  right=0mm,
  colback=white,
  coltitle=black,
  fonttitle=\bfseries\sffamily,
  %separator sign=\raisebox{-0.65ex}{\Large\MI\symbol{58828}},
  description font=\itshape,
  nameref/.style={},
}{ejer}

% CÓDIGO
\usepackage{listings}

% CABECERAS
\pagestyle{headings}
\setkomafont{pageheadfoot}{\normalfont\normalcolor\sffamily\small}
\setkomafont{pagenumber}{\normalfont\sffamily}

% ALGORITMOS
\usepackage[vlined,linesnumbered]{algorithm2e}

% Formato de los pies de figura
\setkomafont{captionlabel}{\scshape}
\SetAlCapFnt{\normalfont\scshape}
\SetAlgorithmName{Algoritmo}{Algoritmo}{Lista de algoritmos}

% BIBLIOGRAFÍA
%\usepackage[sorting=none]{biblatex}
%\addbibresource{bibliografia.bib}

\begin{document}

\renewcommand{\proofname}{\normalfont\sffamily\bfseries\small DEMOSTRACIÓN}

\title{Problema del Aprendizaje de Pesos}
\subject{Metaheurísticas}
\author{Guillermo Galindo Ortuño\\
    4 del Doble Grado en Informática y Matemáticas\\
    Guillegalor@gmail.com\\
    25619527b\\
    Problema 1.b APC}
\publishers{\vspace{2cm}\includegraphics[height=2.5cm]{UGR}\vspace{1cm}}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Descripción del Problema}

El objetivo de esta práctica es, a partir de un conjunto de muestras de elementos ya clasificados, construir un algoritmo que utilizando la información de la que dispone sea capaz de clasificar nuevos elementos. \\

Formalmente, dado un $n \in \mathbb{N}$, y un conjunto de clases $C$, un clasificador es una funció $f$ definida de  $\mathbb{R}^n$ en $C$ que asigna a cada vector $x \in \mathbb{R}^n$ una clase $f(x)$.\\

Nuestro problema consiste entonces en dada un conjunto finito de elementos ya clasificados $A = \{(x,c(x) \in \mathbb{R}^n \times C)$, encontrar un clasificador que permita clasificar otros elementos utilizando la información disponible en $A$. \\

En nuestro caso, partimos del clasificador 1-NN, que consiste en elegir la clase del elemento más cercano del conjunto ya conocido, utilizando usualmente la distancia euclídea. En particular, nosotros utilizaremos una distancia euclídea ponderada, que consiste en, dados dos vectores $u, v \in \mathbb{R}^n$ y un vector de pesos $w \in [0,1]^n$:
\[
d_w(u,v) = \sqrt{\sum_{i = 1}^{n}w*(u_i - v_i)^2}
.\]

Dicho esto, nuestro trabajo será estudiar diversos algoritmos que generen un vector de pesos y analizar como se comportan los respectivos clasificadores \newpage

\section{Consideraciones previas}
\subsection{Esquemas de representación}
\subsubsection{Práctica 1}
En primer lugar, para representar cada tipo de dato distinto utilizamos una interfaz llamada DataElem, de manera que cualquier estructura de datos que implemente DataElem deberá tener definidos los siguientes métodos:
\begin{itemize}
    \item \texttt{\textbf{fn} new()}. Función de clase que devuelve un elemento por defecto
    \item \texttt{\textbf{fn} get\_num\_attributes().} Función de clase que devuelve el número de atributos que tiene dicho tipo de dato.
    \item \texttt{\textbf{fn} get\_id()}  Función que devuelve un identificador de un elemento.
    \item \texttt{\textbf{fn} get\_class()} Función que devuelve la clase de un elemento.
    \item \texttt{\textbf{fn} get\_attr(i)} Función que devuelve el atributo i-ésimo de un elemento.
    \item \texttt{\textbf{fn} set\_id(i)} Función que asigna al identificador de un elemento el valor indicado en el parámetro \texttt{i}.
    \item \texttt{\textbf{fn} set\_class(c)} Función que asigna a la clase de un elemento el valor indicado en el parámetro \texttt{c}.
    \item \texttt{\textbf{fn} set\_attribute(i, v)} Función que asigna al atributo i-ésimo de un elemento el valor indicado en el parámetro \texttt{a}.
\end{itemize}

Dicho esto, hemos utilizado tres estructuras de datos que implementan esta interfaz:
\begin{itemize}
    \item \texttt{TextureRecord}, la cuál contiene un identificador, un array con 40 valores reales para almacenar los atributos, y un entero que representa la clase de dicho elemento.
    \item \texttt{ColposcopyRecord}, la cuál contiene un identificador, un array con 62 valores realespara almacenar los atributos, y un entero que representa la clase de dicho elemento.
    \item \texttt{TextureRecord}, la cuál contiene un identificador, un array con 34 valores reales para almacenar los atributos, y un entero que representa la clase de dicho elemento.
\end{itemize}
\subsubsection{Práctica 2}
En esta práctica hemos definido el tipo \texttt{Chromosome} que es un vector de pesos, y además hemos definido una estructura \texttt{ChromosomeAndResult} que contiene un \texttt{Chromosome} y una variable \texttt{result}, que es un flotante que almacena la evaluación de dicho vector de pesos. Sobre esta estructura definimos un orden que consiste en comparar los resultados de ambos elementos.
\subsection{Operadores comunes}
\subsubsection{Práctica 1}
Dados dos elementos de un tipo de dato que implemente dicha interfaz, hemos definido la función que calcula la \textbf{distancia euclídea} ponderada por un vector de pesos de la siguiente forma:

\begin{verbatim}

euclidean_distance_with_weigths(elem_1, elem_2, weights[]){
    distance = 0;

    for ind in 0 to num_attributtes {
        distance += (elem_1.attributes(ind) - elem_2.attributes(ind))
            * (elem_1.attributes(ind) - elem_2.attributes(ind))
            * weights[ind];
    }

    distance = square_root(distance);

    return distance;
}

\end{verbatim}
Por comodidad, cuando nos refiramos a la distancia euclídea usual omitiremos que llamamos a esta función con un vector de pesos con todos los valores iguales a 1 y únicamente escribiremos \texttt{euclidean\_distance} \\

Utilizando dicha distancia, definimos la función que actua como el \textbf{clasificador 1nn} así:
\begin{verbatim}
classifier_1nn_with_weights(data[], item, weights[]) {
    class_min = data[0].class;
    dist_min = eu_dist_with_weigths(item, data[0], weights);

    // Discards weights lower than 0.2
    for w in weights {
        if w < 0.2 {
            w = 0.0;
        }
    }

    for example in data {
        if example.id != item.id {
            aux_distance = euclidean_distance_with_weigths
                (item, example, weights);
            if aux_distance < d_min {
                class_min = example.class;
                dist_min = aux_distance;
            }
        }
    }

    return c_min;
}

\end{verbatim}

Además hemos definido dos funciones más que toman como argumento un conjunto de elementos de un tipo que implemente \texttt{DataElem}.\\

La primera es \texttt{normalize\_data}, la cual devuelve un conjunto con el mismo número de elementos que el original, con la diferencia de que en este nuevo conjunto todos los elementos tienen sus atributos normalizados respecto al conjunto original. \\

La segunda es \texttt{make\_partitions}, la cual devuelve un vector con 5 conjuntos de elementos que representan 5 particiones, que mantienen en la medida de lo posible la distribución de clases del conjunto original.\\

\subsubsection{Práctica 2}
Para esta práctica hemos definido cuatro operadores, uno de selección, dos de cruce y uno de mutación. Estos se los pasaremos posteriormente a los algoritmos principales, permitiendo en un futuro definir un operador arbitrario y poder ejecutar los algoritmos sin tener que modificar la función principal.

El primer operador que hemos definido es \textbf{binary\_tournament\_selection}, que realiza la selección por torneo binario. Este toma como argumentos el conjunto de \texttt{ChromosomeAndResult} del que eligirá a los posibles padres, el número de padres que queremos generar, y por último el rng para controlar la generación aleatoria.

\begin{verbatim}
binary_tournament_selection(population[], num_parents, rng)  {
    population_size = population.size();
    parents[num_parents];

    for _ in 0..num_parents {
        selector =
            rng.gen_range(0, population_size * population_size);
        first_competitor = population[selector / population_size];
        second_competitor = population[selector % population_size];

        if first_competitor > second_competitor {
            parents.push(first_competitor);
        } else {
            parents.push(second_competitor);
        }
    }

    return parents;
}

\end{verbatim}

El siguiente operador que definimos es \textbf{arithmetic\_mean\_crossover}, que toma como argumentos dos \texttt{Chromosome} y un porcentaje \texttt{alpha}. Inicialmente este operador devolvía un único hijo que era la combinación convexa entre los dos padres al 50\%. Sin embargo, para ayudar a la abstracción del problema lo hemos definido de manera que puedas elegir el porcentaje que mantiene de cada uno de los padres. Así, devuelve dos hijos, uno que posea un porcentaje $\alpha$ de un padre y  un porcentaje $1-\alpha$ de otro y viceversa.

\begin{verbatim}
arithmetic_mean_crossover(one, other, alpha) {
    chromosomes[2];

    for gene in 0..one.size() {
        chromosomes[0]
            .push(one[gene] * (alpha) + other[gene] * (1. - alpha));
        chromosomes[1]
            .push(one[gene] * (1. - alpha) + other[gene] * (alpha));
    }

    return chromosomes;
}

\end{verbatim}

A continuación definimos otro operador de cruce, \textbf{blx\_alpha\_crossover}. Este toma como argumentos dos \texttt{Chromosome} al igual que al anterior, otro porcentaje \texttt{alpha} , y un rng.

\begin{verbatim}
blx_alpha_crossover(one, other, alpha, rng) {
    chromosomes[2];

    for gene in 0..one.size() {
        c_max;
        c_min;

        if one[gene] < other[gene] {
            c_max = other[gene];
            c_min = one[gene];
        } else if other[gene] < one[gene] {
            c_max = one[gene];
            c_min = other[gene];
        } else {
            chromosomes[0].push(one[gene]);
            chromosomes[1].push(one[gene]);

            continue;
        }

        dist = c_max - c_min;

        gene_value1 =
            rng.gen_range(c_min - dist * alpha, c_max + dist * alpha);
        gene_value2 =
            rng.gen_range(c_min - dist * alpha, c_max + dist * alpha);

        // Truncate into [0,1] gene_value1
        if gene_value1 < 0. {
            gene_value1 = 0.;
        } else if gene_value1 > 1. {
            gene_value1 = 1.;
        }

        // Truncate into [0,1] gene_value2
        if gene_value2 < 0. {
            gene_value2 = 0.;
        } else if gene_value2 > 1. {
            gene_value2 = 1.;
        }

        chromosomes[0].push(gene_value1);
        chromosomes[1].push(gene_value2);
    }

    chromosomes
}

\end{verbatim}

% TODO BUSCAR NOMBRE DEL OPERADOR
Por último, el operador de mutación $Mov(\mu, \sigma^2)$ que ya utilizamos en la práctica uno lo hemos definido en una función a parte, con el fin de evitar repiticiones.

\begin{verbatim}
weight_mutation(&weights, index, mean, std_deviation, rng) {
    // Normal distribution with determined mean and standard deviation
    normal = Normal(mean, std_deviation);

    weights[index] += normal.sample(rng);

    // Truncate into [0,1]
    if weights[index] < 0. {
        weights[index] = 0.;
    } else if weights[index] > 1. {
        weights[index] = 1.;
    }
}
\end{verbatim}
\subsection{Función objetivo}
Fijado un problema de clasificación, para evaluar el rendimiento de un vector de pesos calculado sobre un conjunto de datos utilizaremos tres funciones:
\begin{itemize}
    \item \texttt{class\_rate}. A partir de la predicción obtenida mediante el clasificador 1nn utilizando dicho vector de pesos, devuelve porcentaje de elementos bien clasificados(tasa de clasficación):
    \[
    class\_rate = 100 \cdot \frac{n}{N}
    .\]
Donde $n$ es el número de elementos bien clasificados, y $N$ es el número de elementos total en el conjunto
    \item \texttt{red\_rate} A partir del vector de pesos, devuelve el número de pesos menores de $0.2$, que son los que no se tendrán en cuenta en el clasificador(tasa de reducción).
    \[
    red\_rate = 100 \cdot \frac{w}{W}
    .\]
Donde $w$ es el número de pesos con valor menor a $0.2$ y $W$ es el número total de pesos.
    \item \texttt{evaluation\_function} A partir de la tasa de clasificación y la tasa de reducción, devuelve una suma ponderada de ambas. En nuestro caso,$\alpha$ vale $0.5$, luego la ponderación es la misma para ambas tasas.
    \[
    evaluation\_function = \alpha \cdot class\_rate + (1-\alpha)\cdot red\_rate
    .\]
\end{itemize}

\newpage
\section{Descripción de los algoritmos}
\subsection{Práctica 1}
\subsubsection{Algoritmo RELIEF (greedy)}

El algoritmo RELIEF itera sobre todos los elementos del conjunto de datos que recibe, así que de ahora en adelante nos referiremos al elemento actual como \texttt{current\_elem}, y hasta que lo mencionemos de nuevo todo el código estará dentro de dicho bucle.
\begin{verbatim}

weights[] = {0};

for current_elem in data {
    ...
}
\end{verbatim}

En primer lugar calcula el \textit{aliado} y el \textit{enemigo} más cercano para cada elemento utilizando la distancia euclídea usual.
\begin{verbatim}
index = 0;
nearest_enemy_index = 0;
nearest_ally_index = 0;
best_enemy_distance = MAX_VALUE;
best_ally_distance = MAX_VALUE;

for candidate in data {
    if elem.id != candidate.id {
        aux_distance = euclidean_distance
            (current_elem, candidate);

        // Ally search
        if elem.class == candidate.class {
            if aux_distance < best_ally_distance {
                best_ally_distance = aux_distance;
                nearest_ally_index = index;
            }
        }
        // Enemy search
        else {
            if aux_distance < best_enemy_distance {
                best_enemy_distance = aux_distance;
                nearest_enemy_index = index;
            }
        }
    }
}

nearest_ally = data[nearest_ally_index];
nearest_enemy = data[nearest_enemy_index];
\end{verbatim}

A continuación, a cada peso le suma la distancia entre el atributo correspondiente del elemento actual(\texttt{current\_elem}) y de su aliado más cercano. Análogamente le resta la distancia respectiva a su enemigo más cercano.

\begin{verbatim}
for attr in 0 to num_attrs {
    attr_ally_dist =
        (elem.attributes(attr) - nearest_ally.attributes(attr)).abs;
    attr_enemy_dist =
        (elem.attributes(attr) - nearest_enemy.attributes(attr)).abs;

    weights[attr] += attr_enemy_dist - attr_ally_dist;
}
\end{verbatim}

Por último, fuera del bucle principal, truncamos los pesos negativos al intervalo $[0,+\infty]$ y luego normalizamos utilizando el máximo de todos los pesos.
\begin{verbatim}
max_weight = weights[0];

for w in weights {
    if w > max_weight
        max_weight = w;
}

for w in weights {
    if w < 0.0 {
        w = 0.0;
    } else {
        w = w / max_weight;
    }
}

return weights;
\end{verbatim}

\subsubsection{Algoritmo de búsqueda local}
En primer lugar, inicializamos todas las variables necesarias, y clasificamos el conjunto de datos utilizando un conjunto de pesos generados aleatoriamente.
\begin{verbatim}
local_search(data[], rng) {

    indexes[];
    // Nos referiremos a estas dos lineas como fill_and_refresh_indexes()
    indexes = {0, ..., num_attrs};
    indexes.shuffle(rng);

    // Local search parameters
    num_of_mutations = 0;
    max_mutations = 15000;
    neighbours_without_mutting = 0;
    max_neighbour_without_muting = 20 * num_attrs;

    normal = Normal(0.0, 0.3);
    uniform = Uniform(0.0, 1.0);

    // Initialize random weights (using normal distribution)
    weights[];
    for ind in 0 to num_attrs {
        weights.push(uniform.sample(rng));
    }

    initial_guessing[];
    // Initialize current guessing
    for elem in data {
        initial_guessing.push(
            classifier_1nn_with_weights(data, elem, weights));
    }
    current_ev_rate = evaluation_function(
        class_rate(data, initial_guessing),
        red_rate(weights),
        0.5,
    );
\end{verbatim}

A continuación, en el bucle principal seleccionamos un índice aleatorio, lo mutamos, y comprobamos si el vector de pesos con la mutación mejora al actual, en cuyo caso actualizamos este y continuamos ejecutando

\begin{verbatim}

    while neighbours_without_mutting < max_neighbour_without_muting
        && num_of_mutations < max_mutations
    {
        aux_weights = weights;

        fill_and_refresh_indexes();
        index = indexes.pop();

        // Mutation
        aux_weights[index] += normal.sample(rng);

        // Truncate into [0,1]
        aux_weights[index] = truncate(0, 1, aux_weights[index]);

        aux_guessing[];
        // Initialize candidate guessing
        for elem in data {
            aux_guessing.push(
                classifier_1nn_with_weights(data, elem, aux_weights));
        }
        aux_ev_rate = evaluation_function(
            class_rate(data, aux_guessing),
            red_rate(aux_weights),
            0.5,
        );

        if aux_ev_rate > current_ev_rate {
            current_ev_rate = aux_ev_rate;
            weights = aux_weights;
            neighbours_without_mutting = 0;
            fill_and_refresh_indexes();
        } else
            neighbours_without_mutting += 1;

        num_of_mutations += 1;
    }

    return weights;
}

\end{verbatim}

\subsection{Práctica 2}
\subsubsection{Algoritmo Genético Generacional}
Los argumentos que acepta esta función son:
\begin{itemize}
    \item \textbf{data} Conjunto sobre el que evaluaremos los pesos.
    \item \textbf{rng} Generador de números aleatorios.
    \item \textbf{population\_size} Tamaño de la población durante la ejecución del algoritmo.
    \item \textbf{crossover\_probability} Probabilidad de cruce.
    \item \textbf{mutation\_probability} Probabilidad de mutación.
    \item \textbf{max\_calls\_to\_eval} Número maximo de llamadas a la función de evaluación
    \item \textbf{selection\_operator} Operador de selección. Debe aceptar como argumentos un vector de \texttt{ChromosomeAndResult}, un entero, un rng y devolver un vector de \texttt{ChromosomeAndResult}.
    \item \textbf{crossover\_operator} Operador de cruce. Debe aceptar como argumentos dos \texttt{Chromosome}, un rng y devolver dos \texttt{Chromosome}.
    \item \textbf{mutation\_operator} Operador de mutación. Debe aceptar como argumentos una referencia a un \texttt{Chromosome}, un entero y un rng.
\end{itemize}
\begin{verbatim}
generational_genetic_algorithm(
    data,
    rng,
    population_size,
    crossover_probability,
    mutation_probability,
    max_calls_to_eval,
    selection_operator(ChromosomeAndResult[], int, StdRng)
        -> ChromosomeAndResult[],
    crossover_operator(Chromosome, Chromosome, StdRng)
        -> Chromosome[2],
    mutation_operator(&Chromosome, int, StdRng)
){
    quick_eval = |weights: &Chromosome| {
        return evaluate(data, data, weights, 0.5).2;
    };
\end{verbatim}

En primer lugar creamos una función lambda que para evaluar de manera cómoda un vector de pesos.
A continuación inicializamos todas las variables, y rellenamos la población de cromosomas generados aleatoriamente. Tras esto, ordenamos la población y almacenamos el mejor cromosoma.

\begin{verbatim}
    generation_counter = 0;

    num_gens_per_chromosome = get_num_attributes();
    uniform = Uniform(0.0, 1.0);
    calls_to_eval = 0;

    // Initialization ----
    current_population[];

    aux_weights[num_gens_per_chromosome];
    // Initialize random weights (using uniform distribution)
    for _ in 0..population_size {
        for _ in 0..num_gens_per_chromosome {
            aux_weights.push(uniform.sample(rng));
        }

        aux_chrom_and_result = ChromosomeAndResult {
            chromosome: aux_weights,
            result: quick_eval(aux_weights),
        };

        calls_to_eval += 1;

        current_population.push(aux_chrom_and_result);

        aux_weights.clear();
    }

    current_population.sort();
    current_best_chrom_res = current_population.last();
\end{verbatim}

A continuación, entramos en el bucle principal del que no saldremos hasta que no agotemos el número de llamadas a la función de evaluación. \\

Aplicamos el operador de selección que nos pasan como argumento y generamos tantos padres como tamaño tenga nuestra población. Ahora, como el operador de cruce siempre nos devuelve dos hijos, cruzamos el número de padres esperado (calculado a partir de la probabilidad de cruce), y añadimos los hijos a la población auxiliar (los añadimos con resultado $-1$ para calcularlo más tarde). Tras esto, añadimos a la población auxiliar el resto de padres hasta alcanzar el tamaño de la población.

\begin{verbatim}
    while calls_to_eval < max_calls_to_eval {

        let mut auxiliar_population: Vec<ChromosomeAndResult> = Vec::with_capacity(population_size);

        // Selection  ----
        parents = selection_operator(current_population, population_size, rng);

        // Crossover (arithmetic mean) ----
        num_of_crossovers = (crossover_probability * population_size ) / 2;

        // Adds children chromosomes
        for index in 0..num_of_crossovers {
            children = crossover_operator(
                parents[2 * index].chromosome,
                parents[2 * index + 1].chromosome,
                rng,
            );

            auxiliar_population.push(ChromosomeAndResult.new(children[0], -1.));
            auxiliar_population.push(ChromosomeAndResult.new(children[1], -1.));
        }

        // Adds remaining chromosomes from parents
        for index in 2 * num_of_crossovers..population_size {
            auxiliar_population.push(parents[index]);
        }
\end{verbatim}

Ahora, para calcular el número de mutaciones a realizar, calculamos el número de mutaciones esperadas sin perder los decimales. Con esto, el número de mutaciones a realizar será la parte de real de dicha esperanza. Para ser más precisos con el número de mutaciones, generaremos un número aleatorio entre $0$ y $1$. Si este es más pequeño que la parte real de la esperanza, sumaremos uno al número de mutaciones a realizar.

\begin{verbatim}
        // Mutation ----
        expected_num_of_mutations =
            mutation_probability * population_size  * num_gens_per_chromosome ;
        num_of_mutations = expected_num_of_mutations as usize;

        if rng.gen_range(0, 1) < (expected_num_of_mutations - num_of_mutations) {
            num_of_mutations += 1;
        }

        for _ in 0..num_of_mutations {
            selector = rng.gen_range(0, population_size * num_gens_per_chromosome);
            chosen_chromosome = auxiliar_population[selector % population_size].chromosome;
            chosen_gene = selector / population_size;

            // Mutation operator
            mutation_operator(chosen_chromosome, chosen_gene, rng);

            auxiliar_population[selector % population_size].result =
                quick_eval(chosen_chromosome);

            calls_to_eval += 1;
        }

\end{verbatim}

Por último evaluamos todos aquellos cromosomas que no estuviesen ya evaluados, ordenamos la población auxiliar y almacenamos el mejor cromosoma de la población auxiliar. Tras esto, comparamos los dos mejores cromosomas y nos quedamos con el mejor de los dos para la nueva población. Actualizamos toda la población y continuamos.

\begin{verbatim}
        // Evaluation ----
        for chrom_and_res in auxiliar_population.iter_mut() {
            if chrom_and_res.result == -1. {
                chrom_and_res.result = quick_eval(&chrom_and_res.chromosome);
                calls_to_eval += 1;
            }
        }

        auxiliar_population.sort();
        let new_best_chrom_res = auxiliar_population.last().unwrap().clone();

        // Replacement ----
        // If new best chrom is worst than the old one
        if new_best_chrom_res < current_best_chrom_res {
            auxiliar_population.remove(0);
            auxiliar_population.push(current_best_chrom_res.clone());
        } else {
            current_best_chrom_res = new_best_chrom_res;
        }

        current_population = auxiliar_population;

        generation_counter += 1;
    }

    return current_best_chrom_res.chromosome;
}
\end{verbatim}

\subsubsection{Algoritmo Genético Estacionario}
Para evitar repeticiones, de este algoritmo únicamente hablaremos de las diferencias con el generacional.
\begin{verbatim}
steady_state_genetic_algorithm(
    data,
    rng,
    population_size,
    crossover_probability,
    mutation_probability,
    max_calls_to_eval,
    selection_operator(ChromosomeAndResult[], int, StdRng)
        -> ChromosomeAndResult[],
    crossover_operator(Chromosome, Chromosome, StdRng)
        -> Chromosome[2],
    mutation_operator(&Chromosome, int, StdRng)
){
    quick_eval = |weights: &Chromosome| {
        return evaluate(data, data, weights, 0.5).2;
    };

    generation_counter = 0;

    num_gens_per_chromosome = get_num_attributes();
    uniform = Uniform(0.0, 1.0);
    calls_to_eval = 0;

    // Initialization ----
    current_population[population_size];

    aux_weights[num_gens_per_chromosome];
    // Initialize random weights (using normal distribution)
    for _ in 0..population_size {
        for _ in 0..num_gens_per_chromosome {
            aux_weights.push(uniform.sample(rng));
        }

        let aux_chrom_and_result = ChromosomeAndResult {
            chromosome: aux_weights,
            result: quick_eval(aux_weights),
        };

        calls_to_eval += 1;

        current_population.push(aux_chrom_and_result);

        aux_weights.clear();
    }

    current_population.sort();
    current_best_chrom_res = current_population.last();

    while calls_to_eval < max_calls_to_eval {
\end{verbatim}
En este algoritmo únicamente necesitaremos dos hijos, así que solo generamos dos padres.
\begin{verbatim}
        // Selection  ----
        let parents = selection_operator(current_population, 2, rng);

        // Crossover  ----
        children =
            crossover_operator(
                parents[0].chromosome,
                parents[1].chromosome, rng
            );
        auxiliar_population[2];
        auxiliar_population.push(
            ChromosomeAndResult.new(children[0], -1));
        auxiliar_population.push(
            ChromosomeAndResult.new(children[1], -1));

        // Mutation ----
        expected_num_of_mutations =
            mutation_probability * population_size  * num_gens_per_chromosome ;
        num_of_mutations = expected_num_of_mutations as usize;

        if rng.gen_range(0, 1) < (expected_num_of_mutations - num_of_mutations) {
            num_of_mutations += 1;
        }

        for _ in 0..num_of_mutations {
            selector = rng.gen_range(0, population_size * num_gens_per_chromosome);
            chosen_chromosome = auxiliar_population[selector % population_size].chromosome;
            chosen_gene = selector / population_size;

            // Mutation operator
            mutation_operator(chosen_chromosome, chosen_gene, rng);

            auxiliar_population[selector % population_size].result =
                quick_eval(chosen_chromosome);

            calls_to_eval += 1;
        }

        for chrom_and_res in auxiliar_population.iter_mut() {
            if chrom_and_res.result == -1. {
                chrom_and_res.result = quick_eval(chrom_and_res.chromosome);
                calls_to_eval += 1;
            }
        }

        auxiliar_population.sort();
\end{verbatim}
Ahora, al reemplazar, comparamos los dos hijos que hemos obtenido con los peores cromosomas de nuestra población actual, y de entre estos cuatro nos quedamos con los dos mejores.
\begin{verbatim}
        // Replacement ----
        // Keeps two best of current_population[0], current_population[1],
        // auxiliar_population[0], auxiliar_population[1]

        // if previous best is worse than new worst replace both previous
        if current_population[1] < auxiliar_population[0] {
            current_population.remove(0);
            current_population.remove(1);
            current_population.push(auxiliar_population[0]);
            current_population.push(auxiliar_population[1]);
        }
        // if not, if previous worst is worse than new best replace only him
        else if current_population[0] < auxiliar_population[1] {
            current_population.remove(0);
            current_population.push(auxiliar_population[1]);
        }

        current_population.sort();
        current_best_chrom_res = current_population.last();

        generation_counter += 1;
    }

    return current_best_chrom_res.chromosome;
}

\end{verbatim}

\subsubsection{Algoritmo Memético}
\newpage
\section{Procedimiento considerado para desarrollar la práctica.}
La implementación de la práctica se ha llevado a cabo en el lenguaje de programación Rust, y no se ha utilizado ningún framework de metaheurísticas.\\

El código se encuentra en \texttt{src/main.rs}. Para compilarlo, en primer lugar es necesario instalar rustup, lo cual se encargará de instalar el propio lenguaje y la herramienta Cargo. Para instalar rustup solo es necesario ejecutar este comando:
\begin{shell}
curl https://sh.rustup.rs -sSf | sh
\end{shell}

Tras esto, para compilar el programa y ejecutarlo tenemos que hacer:
\begin{shell}
cargo build --release
cargo run --release <args>
\end{shell}

El único argumento que acepta el programa es un entero, que será el que actuará como semilla para cualquier valor aleatorio sea necesario en el programa. Si se ejecuta sin argumentos, se toma como semilla por defecto el valor $1$.

El programa ejecuta todos los algoritmos, y muestra por la salida estándar en formato tabla los resultados de cada algoritmo para los 3 conjuntos de datos proporcionados en la carpeta \texttt{data}.

\newpage

\section{Experimentos y análisis de los resultados}
\subsection{Descripción de los casos del problema}
\subsubsection{Colposcopy}
Colposcopy es una base de datos sobre un procedimiento ginecológico que consiste en la exploración del cuello uterino. El fichero proporcionado contiene $287$ elementos que representan una imagen de la secuencia colposcópica.\\

Cada ejemplo consta de 62 atributos reales(representan características extraidas de las imágenes) y una clase que puede ser de dos tipos, positivo o negativo.

\subsubsection{Ionosphere}
Ionosphere es una base de datos de radar recogidos por un sistema en Goose Bay, Labrador. Este sistema consiste en un conjunto de fases de 16 antenas de alta frecuencia con una potencia total transmitida del orden de 6,4 kilovatios. El fichero proporcionado contiene $352$ elementos.

Cada ejemplo consta de 34 atributos reales y de una clase que puede ser bueno (si el retorno de radar muestra evidencia de algún tipo de estructura en la ionosfera) o malo (las señales simplemente atraviesan la ionosfera).

\subsubsection{Texture}
Texture es una base de datos que contiene datos sobre fotografias de distintos tipos de texturas con el objetivo de distinguir entre 11 texturas diferentes (césped, piel de becerro prensada, papel hecho a
mano, rafia en bucle a una pila alta, lienzo de algodón,...). El fichero proporcionado contiene $550$ ejemplos.

Cada ejemplo consta de 40 atributos construidos mediante la estimación de momentos modificados de cuarto orden en cuatro orientaciones: 0, 45, 90 y 135 grados, y de una clase que indica el tipo de textura.

\subsection{Resultados obtenidos}
En esta sección mostraremos los resultados obtenidos al ejecutar cada uno de los algoritmos sobre cada conjunto de datos proporcionado. Mostraremos los resultados en tablas, una por cada algoritmo y conjunto de datos. Cada tabla posee 5 columnas:
\begin{itemize}
    \item \textbf{Partición}. Indica que partición de las 5 se está utilizando como conjunto de prueba.
    \item \textbf{Tasa\_class}. Tasa de clasificación obtenida.
    \item \textbf{Tasa\_red}. Tasa de reducción obtenida.
    \item \textbf{Agregado}. Función de evaluación utilizando los valores anteriores.
    \item \textbf{Tiempo}. Tiempo de ejecución del algoritmo en milisegundos.
\end{itemize}

La ejecución que mostraremos se ha realizado utilizando como semilla el valor por defecto $1$. La ejecución ha sido realizado en mi portatil personal con sistema operativo Manjaro Linux 18.0.3, y procesador Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\\

En plas primeras seis tablas (1-6) se muestran los resultados de dos algoritmos aleatorios distintos: el primero elige una clase aleatoria de entre las conocidas para clasificar y el segundo crea un vector de pesos aleatorio y clasifica de manera normal utilizandolo

En las siguientes tres tablas (7-9) se muestran los resultados obtenidos utilizando simplemente el algoritmo \textbf{1nn} para clasificar. A continuación, se muestran los resultados asociados al algoritmo greedy \textbf{RELIEF} (tablas 10-12). Por último, las tres tablas restantes (13-15) muestran los resultados obtenidos utilizando la \textbf{búsqueda local} como algoritmo para resolver el problema de APC.

\newpage
\subsubsection{Tablas Selección Aleatoria}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 64.40678  & 100        & 82.20339  & 0      \\
1         & 59.649124 & 100        & 79.824562 & 0      \\
2         & 70.17544  & 100        & 85.08772  & 0      \\
3         & 61.403507 & 100        & 80.701754 & 0      \\
4         & 57.894737 & 100        & 78.947369 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 50.704224 & 100        & 75.352112 & 0      \\
1         & 45.714287 & 100        & 72.857143 & 0      \\
2         & 45.714287 & 100        & 72.857143 & 0      \\
3         & 60        & 100        & 80.0      & 0      \\
4         & 61.42857  & 100        & 80.714285 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 9.090909  & 100        & 54.5454545 & 0      \\
1         & 7.2727275 & 100        & 53.636364  & 0      \\
2         & 6.3636365 & 100        & 53.1818182 & 0      \\
3         & 11.818182 & 100        & 55.909091  & 0      \\
4         & 9.090909  & 100        & 54.5454545 & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de seleccción aleatoria sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas Pesos Aleatorios}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 76.27119  & 22.580645  & 49.425915 & 0      \\
1         & 70.17544  & 20.967741  & 45.57159  & 0      \\
2         & 70.17544  & 14.5161295 & 42.345783 & 0      \\
3         & 71.929825 & 29.032259  & 50.48104  & 0      \\
4         & 77.192986 & 17.741936  & 47.46746  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 17.647058 & 53.89395  & 0      \\
1         & 81.42857  & 26.470589 & 53.94958  & 0      \\
2         & 82.85714  & 17.647058 & 50.252098 & 0      \\
3         & 94.28571  & 26.470589 & 60.37815  & 0      \\
4         & 87.14286  & 17.647058 & 52.39496  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 95.454544 & 10       & 52.727272 & 1      \\
1         & 90        & 25       & 57.5      & 1      \\
2         & 96.36364  & 27.5     & 61.93182  & 1      \\
3         & 90        & 20       & 55        & 1      \\
4         & 93.63636  & 20       & 56.81818  & 1      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución de pesos aleatorios sobre el conjunto Texture}
  \end{table}%

\newpage
\subsubsection{Tablas 1nn}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 74.57627  & 0        & 37.288136 & 0      \\
1         & 70.17544  & 0        & 35.08772  & 0      \\
2         & 73.68421  & 0        & 36.842106 & 0      \\
3         & 75.4386   & 0        & 37.7193   & 0      \\
4         & 82.45614  & 0        & 41.22807  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 0        & 45.070423 & 0      \\
1         & 80        & 0        & 40        & 0      \\
2         & 82.85714  & 0        & 41.42857  & 0      \\
3         & 92.85714  & 0        & 46.42857  & 0      \\
4         & 87.14286  & 0        & 43.57143  & 0      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 93.63636  & 0        & 46.81818  & 1      \\
1         & 89.09091  & 0        & 44.545456 & 1      \\
2         & 94.545456 & 0        & 47.272728 & 1      \\
3         & 92.72727  & 0        & 46.363636 & 1      \\
4         & 92.72727  & 0        & 46.363636 & 1      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del \textbf{1nn} sobre el conjunto Texture}
  \end{table}%

\newpage
\subsubsection{Tablas RELIEF}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 72.881355 & 40.322582 & 56.601967 & 3      \\
1         & 75.4386   & 27.419355 & 51.428978 & 3      \\
2         & 77.192986 & 32.258064 & 54.725525 & 3      \\
3         & 71.929825 & 51.612904 & 61.771362 & 3      \\
4         & 82.45614  & 30.64516  & 56.55065  & 3      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.14085  & 2.9411764 & 46.54101  & 2      \\
1         & 81.42857  & 2.9411764 & 42.184875 & 2      \\
2         & 82.85714  & 2.9411764 & 42.89916  & 2      \\
3         & 92.85714  & 2.9411764 & 47.89916  & 2      \\
4         & 90        & 2.9411764 & 46.47059  & 2      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 91.818184 & 15       & 53.409092 & 7      \\
1         & 90.90909  & 2.5      & 46.704544 & 7      \\
2         & 95.454544 & 2.5      & 48.977272 & 7      \\
3         & 92.72727  & 2.5      & 47.613636 & 7      \\
4         & 93.63636  & 5        & 49.31818  & 7      \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{RELIEF} sobre el conjunto Texture}

  \end{table}%

\newpage

\subsubsection{Tablas Busqueda Local}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 72.881355 & 85.48387  & 79.18262  & 11806  \\
1         & 71.929825 & 80.645164 & 76.28749  & 6312   \\
2         & 68.42105  & 79.03226  & 73.726654 & 8371   \\
3         & 68.42105  & 80.645164 & 74.53311  & 15446  \\
4         & 73.68421  & 77.41936  & 75.55179  & 13647  \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{busqueda local} sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 91.54929  & 88.23529 & 89.89229  & 3995   \\
1         & 85.71429  & 85.29412 & 85.5042   & 2753   \\
2         & 84.28571  & 91.17647 & 87.731094 & 4425   \\
3         & 91.42857  & 85.29412 & 88.36134  & 2889   \\
4         & 84.28571  & 82.35294 & 83.31933  & 2318   \\
  \bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{busqueda local} sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{rrrrr}
  \toprule
  Partición &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
0         & 90.90909  & 82.5     & 86.704544 & 17734  \\
1         & 89.09091  & 75       & 82.045456 & 14127  \\
2         & 90.90909  & 85       & 87.954544 & 20305  \\
3         & 88.181816 & 80       & 84.09091  & 8280   \\
4         & 87.27273  & 90       & 88.63637  & 27707  \\
\bottomrule
  \end{tabular}
  \caption{Ejecución del algoritmo \textbf{busqueda local} sobre el conjunto Texture}
  \end{table}%

\newpage

\subsubsection{Tablas Agrupadas}

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random             & 62.705918 & 100 & 81.352959 & 0\\
 Random weights     & 73.148976 & 20.967742 & 47.058359 & 0\\
 1-NN               & 75.266132 & 0 & 37.633066 & 0\\
 RELIEF             & 75.979781 & 36.451613 & 56.215697 & 3\\
 Local search       & 71.067498 & 80.645164 & 75.856331 & 11073.8\\
  \bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Colposcopy }
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random & 52.712274 & 100 & 76.356137 & 0\\
 Random weights & 87.171026 & 21.176470 & 54.173748 & 0\\
1-NN & 86.599598 & 0 & 43.299799 & 0\\
 RELIEF& 87.45674 & 2.9411764 & 45.198958 & 2\\
 Local search& 87.452714 & 86.470588 & 86.961651 & 3235.6\\
  \bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Ionosphere}
  \end{table}%

 \begin{table}[ht]
  \centering
  \begin{tabular}[t]{lrrrr}
  \toprule
  Algoritmo &Tasa\_class &Tasa\_red & Agregado & Tiempo(ms)\\
  \midrule
 Random             & 8.7272728 & 100 & 54.363637 & 0\\
 Random weights     & 93.090909 & 20.5 & 56.795454 & 1\\
 1-NN               & 92.545453 & 0 & 46.272727 & 1\\
 RELIEF             & 92.909090 & 5.5 & 49.204545 & 7.2\\
 Local search       & 89.272727 & 82.5 & 85.886364 & 17632.4\\
\bottomrule
  \end{tabular}
  \caption{Tabla conjunta sobre el conjunto Texture}
  \end{table}%

\newpage

\subsection{Análisis de los resultados}

Comencemos hablando del algoritmo que selecciona una clase de las conocidas de manera aleatoria. Al contrario de lo que cabría esperar, obtiene <<buenos>> resultados tanto sobre el conjunto de datos \textit{Colposcopy}, como sobre \textit{Ionosphere} . Esto es debido a que únicamente hay dos clases distintas, luego la tasa de clasificación está cercana al $50\%$ , y además la tasa de reducción es del $100\%$ ya que no utiliza ningún peso. En consecuencia el agregado se encuentra entre el $70\%$ y el $80\%$. Ahora bien, como en \textit{Texture} hay once clases distintas, la tasa de clasificación es bastante mala pero el agregado sigue estando por encima del $50\%$ gracias a la tasa de reducción.

Si nos fijamos ahora en el algoritmo que utiliza un vector de pesos aleatorios, observamos que tanto en el conjunto \textit{Ionosphere} como en \textit{Texture} obtiene muy buenas tasas de clasificación (cercanas al $90\%$), y bastante buenas en \textit{Colposcopy} (cerca de un $73\%$). Sin embargo, la tasa de reducción no es muy grande en ninguno de los casos, estando siempre en torno al $20\%$. Esto provoca que el agregado esté cerca del $50\%$ en todos los casos.

Dicho esto, fijándonos en la tabla del \textbf{1-NN}, en los tres casos obtenemos resultados muy parecidos a los obtenidos utilizando un vector de pesos aleatorio, sin embargo en este caso como tenemos en cuenta todos los pesos (la distancia euclidia toma todos los pesos a $1$), la tasa de reducción es $0$ siempre, lo que se ve reflejado en el agregado, que es aproximadamente un $10\%$ menor que cuando utilizamos un vector de pesos aleatorio.

En todos los algoritmos anteriores, como prácticamente la única operación que hay que hacer es calcular la distancia entre todos los elementos una vez, y los conjuntos no son muy grandes, los tiempos de ejecución son practicamente despreciables.

Si nos fijamos en los resultados obtenidos por uno de los dos algoritmos principales de esta práctica, el \textbf{RELIEF}, las tasas de clasificación son ligeramente mayores o practicamente iguales que en los dos casos anteriores. Sin tener en cuenta el conjunto colposcopy, obtenemos tasas de reducciones muy pequeñas aunque no nulas, provocando que el agregado sea mayor que el del \textbf{1-NN} aunque menor que el vector de pesos aleatorio. En este algoritmo ya observamos que hemos medido un tiempo, aunque bastante pequeño, significativo ya que es necesario recorrer dos veces el conjunto de elementos (una para calcular los enemigos y aliados y calcular los pesos, y otra para ejecutar \textbf{1-NN}).

Por último, si estudiamos la \textbf{búsqueda local} es facil observar que es la que mejores resultados obtiene si tenemos en cuenta la función de evaluación. Esto es debido a que en la implementación de este algoritmo, se buscan maximos de la función de evaluación, por tanto, aunque las tasas de clasificación sean ligeramente menores que en los tres casos anteriores, se consigue una muy alta tasa de reducción, lo que consigue que el agregado esté cercano al $75\%$ en Colposcopy, y al  $89\%$ en los dos conjuntos restantes. Si hablamos del tiempo, en este algoritmo es considerablemente mayor, lo cual era de esperar, pues en cada iteración del bucle principal se evaluán los pesos tras la mutación, por lo que se recorre el conjunto de elementos un mínimo de veinte veces el número de atributos y un máximo de 15000 veces.

Las conclusiones que obtenemos teniendo en cuenta todos los resultados es que, en la función de evaluación le estamos dando demasiada importancia a la reducción, ya que ambos algoritmos aleatorios obtienen prácticamente las puntuaciones más altas a excepción de la \textbf{búsqueda local}  . Esta se comporta como podría esperarse, y es que obtiene puntuaciones muy altas, reduciendo un gran número de pesos.

\end{document}
